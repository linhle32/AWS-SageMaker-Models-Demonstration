{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36ef6bbb",
   "metadata": {},
   "source": [
    "# SageMaker AutoPilot\n",
    "\n",
    "SageMaker provides a feature called **AutoPilot** that allows automated training and finetuning of models for various tasks. However, AutoPilot is much very powerful in that it has SageMaker models implemented, as well as automatically process data and engineer new useful features. AutoPilot can also automatically detect whether the task is regression or classification by analyzing the target!\n",
    "\n",
    "In this notebook, we will use AutoPilot on the `obesity.csv` data. Interestingly, since the target in this data is integer and only has seven values (1 to 7), AutoPilot will consider this task classification. In data with continuous target, AutoPilot will run regression models.\n",
    "\n",
    "## Load data\n",
    "\n",
    "Not much to discuss here, we set the path to the bucket and the data, and load it into a `pandas` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a07f770e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/fsspec/registry.py:272: UserWarning: Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n",
      "  warnings.warn(s3_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>FamilyHistory</th>\n",
       "      <th>FAVC</th>\n",
       "      <th>FCVC</th>\n",
       "      <th>NCP</th>\n",
       "      <th>CAEC</th>\n",
       "      <th>SMOKE</th>\n",
       "      <th>CH2O</th>\n",
       "      <th>SCC</th>\n",
       "      <th>FAF</th>\n",
       "      <th>TUE</th>\n",
       "      <th>CALC</th>\n",
       "      <th>MTRANS</th>\n",
       "      <th>ObesityLevel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Female</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.62</td>\n",
       "      <td>64.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>no</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Female</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.52</td>\n",
       "      <td>56.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>77.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sometimes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>no</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Frequently</td>\n",
       "      <td>Public_Transportation</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender   Age  Height  Weight FamilyHistory FAVC  FCVC  NCP       CAEC  \\\n",
       "0  Female  21.0    1.62    64.0           yes   no   2.0  3.0  Sometimes   \n",
       "1  Female  21.0    1.52    56.0           yes   no   3.0  3.0  Sometimes   \n",
       "2    Male  23.0    1.80    77.0           yes   no   2.0  3.0  Sometimes   \n",
       "\n",
       "  SMOKE  CH2O  SCC  FAF  TUE        CALC                 MTRANS  ObesityLevel  \n",
       "0    no   2.0   no  0.0  1.0          no  Public_Transportation             2  \n",
       "1   yes   3.0  yes  3.0  0.0   Sometimes  Public_Transportation             2  \n",
       "2    no   2.0   no  2.0  1.0  Frequently  Public_Transportation             2  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_name = 'obesity.csv'\n",
    "data_location = 'your bucket'\n",
    "\n",
    "data = pd.read_csv(data_location + data_name)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b38313",
   "metadata": {},
   "source": [
    "Next, we need to define the input features and the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef943ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'Gender',\n",
    "    'Age',\n",
    "    'Height',\n",
    "    'Weight',\n",
    "    'FamilyHistory',\n",
    "    'FAVC',\n",
    "    'FCVC',\n",
    "    'NCP',\n",
    "    'CAEC',\n",
    "    'SMOKE',\n",
    "    'CH2O',\n",
    "    'SCC',\n",
    "    'FAF',\n",
    "    'TUE',\n",
    "    'CALC',\n",
    "    'MTRANS'\n",
    "]\n",
    "\n",
    "target = 'ObesityLevel'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e3f954",
   "metadata": {},
   "source": [
    "Then split train/test and upload back to our S3 bucket for the model. We also need to drop the target from the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eb8b4467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "traindata, testdata_w_target = train_test_split(data, test_size = 0.2)\n",
    "traindata.to_csv('train_data.csv', index=False)\n",
    "\n",
    "testdata = testdata_w_target.drop([target], axis=1)\n",
    "testdata.to_csv('test_data_no_target.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8710f6",
   "metadata": {},
   "source": [
    "Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7a16a9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "sess = sagemaker.Session()                        # get our current SageMaker session\n",
    "bucket = 'lle13-it7143'                           # this should be the name of the bucket we created in module 9\n",
    "prefix = 'obesity'                                # the folder to store your data in the S3 instance\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train_data.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'test/test.csv')).upload_file('test_data_no_target.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e162f9c",
   "metadata": {},
   "source": [
    "## Set up the AutoPilot Job\n",
    "\n",
    "Since we set up all the required information previously (such as bucket, folder, target), we can run the cell below as-is. Basically, it just sets the data path and target in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc00fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_config = [\n",
    "    {\n",
    "        \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"S3DataType\": \"S3Prefix\",\n",
    "                \"S3Uri\": \"s3://{}/{}/train\".format(bucket, prefix),\n",
    "            }\n",
    "        },\n",
    "        \"TargetAttributeName\": target,\n",
    "    }\n",
    "]\n",
    "\n",
    "job_config = {\"CompletionCriteria\": {\"MaxCandidates\": 10}}\n",
    "\n",
    "\n",
    "output_data_config = {\"S3OutputPath\": \"s3://{}/{}/output\".format(bucket, prefix)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d14866f",
   "metadata": {},
   "source": [
    "## Create and Launch the AutoPilot Job\n",
    "\n",
    "Now we can create and send the auto job to SageMaker. You can see that the code is much simpler compared to XGBoost or Linear Learner since we don't have to set up parameters or set up model images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14a9ed06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AutoMLJobArn': 'arn:aws:sagemaker:us-east-2:111098985325:automl-job/automl-obesity',\n",
       " 'ResponseMetadata': {'RequestId': '4525ca03-4df0-490b-9876-7baa4feaa068',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '4525ca03-4df0-490b-9876-7baa4feaa068',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '85',\n",
       "   'date': 'Sat, 06 Jan 2024 07:32:55 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_ml_job_name = 'automl-obesity'\n",
    "region = boto3.Session().region_name\n",
    "sm = boto3.Session().client(service_name=\"sagemaker\", region_name=region)\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "sm.create_auto_ml_job(\n",
    "    AutoMLJobName=auto_ml_job_name,\n",
    "    InputDataConfig=input_data_config,\n",
    "    OutputDataConfig=output_data_config,\n",
    "    AutoMLJobConfig=job_config,\n",
    "    RoleArn=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe3b707",
   "metadata": {},
   "source": [
    "The AutoPilot job will actually runs separately from this notebook, so you will see the cell above as completed right after running it. To track the job status, run the cell below to retrieve the training information every 60 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b9e6919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobStatus - Secondary Status\n",
      "------------------------------\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - MergingAutoMLTaskReports\n",
      "InProgress - MergingAutoMLTaskReports\n",
      "InProgress - MergingAutoMLTaskReports\n",
      "InProgress - MergingAutoMLTaskReports\n",
      "InProgress - MergingAutoMLTaskReports\n",
      "InProgress - MergingAutoMLTaskReports\n",
      "InProgress - MergingAutoMLTaskReports\n",
      "InProgress - MergingAutoMLTaskReports\n",
      "InProgress - MergingAutoMLTaskReports\n",
      "Completed - Completed\n"
     ]
    }
   ],
   "source": [
    "print(\"JobStatus - Secondary Status\")\n",
    "print(\"------------------------------\")\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "describe_response = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)\n",
    "print(describe_response[\"AutoMLJobStatus\"] + \" - \" + describe_response[\"AutoMLJobSecondaryStatus\"])\n",
    "job_run_status = describe_response[\"AutoMLJobStatus\"]\n",
    "\n",
    "while job_run_status not in (\"Failed\", \"Completed\", \"Stopped\"):\n",
    "    describe_response = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)\n",
    "    job_run_status = describe_response[\"AutoMLJobStatus\"]\n",
    "\n",
    "    print(\n",
    "        describe_response[\"AutoMLJobStatus\"] + \" - \" + describe_response[\"AutoMLJobSecondaryStatus\"]\n",
    "    )\n",
    "    sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22b744c",
   "metadata": {},
   "source": [
    "We can investigate the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea86dad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CandidateName: automl-obesityvSlg1ey6faHtNtWQEk-008-26f090ee\n",
      "metric: validation:accuracy\n",
      "value: 0.9541900157928467\n"
     ]
    }
   ],
   "source": [
    "best_candidate = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)[\"BestCandidate\"]\n",
    "best_candidate_name = best_candidate[\"CandidateName\"]\n",
    "\n",
    "print(\"CandidateName: \" + best_candidate_name)\n",
    "print(\n",
    "    \"metric: \"\n",
    "    + best_candidate[\"FinalAutoMLJobObjectiveMetric\"][\"MetricName\"]\n",
    ")\n",
    "print(\n",
    "    \"value: \"\n",
    "    + str(best_candidate[\"FinalAutoMLJobObjectiveMetric\"][\"Value\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d819887",
   "metadata": {},
   "source": [
    "## Deploy the Selected Model\n",
    "\n",
    "Like before, after training, we need to deploy our model to an endpoint. This time, we need to manually create the endpoint config as well (this was automated previously). However, the code does not require modification and can be reused on different data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f81cb972",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = best_candidate_name + \"-model\"\n",
    "model_arn = sm.create_model(Containers=best_candidate[\"InferenceContainers\"], ModelName=model_name, ExecutionRoleArn=role)\n",
    "\n",
    "epc_name = best_candidate_name + \"-epc\"\n",
    "ep_config = sm.create_endpoint_config(\n",
    "    EndpointConfigName=epc_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.m4.xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": model_name,\n",
    "            \"VariantName\": \"main\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "ep_name = best_candidate_name + \"-ep\"\n",
    "create_endpoint_response = sm.create_endpoint(EndpointName=ep_name, EndpointConfigName=epc_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe08d6e",
   "metadata": {},
   "source": [
    "Another difference compared to previous module is that this time the deployment happens separately from our notebook session. So, we need to run the code below to track when the deployment completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3571c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.get_waiter(\"endpoint_in_service\").wait(EndpointName=ep_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe19c615",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Finally, we can make prediction for the test data. The steps are like in XGBoost and Linear Learner. We convert the test data to a `numpy` array directly from the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "be013484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=ep_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=CSVSerializer(),\n",
    "    deserializer=CSVDeserializer(),\n",
    ")\n",
    "\n",
    "test_data_np = pd.read_csv(f's3://{bucket}/{prefix}/test/test.csv').values\n",
    "prediction = predictor.predict(test_data_np)\n",
    "prediction_df = pd.DataFrame(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff26fd2",
   "metadata": {},
   "source": [
    "In this case, we have the test data labels from `testdata_w_target`, so we can evaluate the model's accuracy and f1. Specifically for this data, the prediction becomes string, so we need to cast it back to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9a56eabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9905437352245863\n",
      "f1 score:  0.9905426939326567\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "prediction_int = np.array(prediction, dtype=int).flatten()\n",
    "print('accuracy: ', accuracy_score(testdata_w_target[target], prediction_int))\n",
    "print('f1 score: ', f1_score(testdata_w_target[target], prediction_int, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0bbec9",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "AutoPilot generates a lot more resources than training and tuning a single model. Therefore, this part becomes a bit more complicated. Regardless, please don't forget to run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "793e1689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrialNames:\n",
      "\n",
      "automl-obesityvSlg1ey6faHtNtWQEk-005-e6f29619-aws-trial\n",
      "\tTrialComponentNames:\n",
      "\tautoml-obesity-dpp2-csv-1-4c1b0cc2291e434aa325268feafc97ac507e9-aws-transform-job\n",
      "\tautoml-obesity-dpp2-1-9a36998be71f446dbd7ea32bcb055e99430bda923-aws-training-job\n",
      "\tautoml-obesity-db-1-097f8c93764e4c6fbb270b8cceff7ceb61bc5bd0c61-aws-processing-job\n",
      "\n",
      "automl-obesityvSlg1ey6faHtNtWQEk-006-3f9b2b0f-aws-trial\n",
      "\tTrialComponentNames:\n",
      "\tautoml-obesityvSlg1ey6faHtNtWQEk-006-3f9b2b0f-aws-training-job\n",
      "\tautoml-obesity-dpp0-csv-1-d7a20225be7d4f1e8679ea2d582e2e564b350-aws-transform-job\n",
      "\tautoml-obesity-dpp0-1-b44a05c57e8a4f58ba3b5c5c597d0e24cc5338710-aws-training-job\n",
      "\tautoml-obesity-db-1-097f8c93764e4c6fbb270b8cceff7ceb61bc5bd0c61-aws-processing-job\n",
      "\n",
      "automl-obesityvSlg1ey6faHtNtWQEk-003-b751cc2f-aws-trial\n",
      "\tTrialComponentNames:\n",
      "\tautoml-obesityvSlg1ey6faHtNtWQEk-003-b751cc2f-aws-training-job\n",
      "\tautoml-obesity-dpp0-csv-1-d7a20225be7d4f1e8679ea2d582e2e564b350-aws-transform-job\n",
      "\tautoml-obesity-dpp0-1-b44a05c57e8a4f58ba3b5c5c597d0e24cc5338710-aws-training-job\n",
      "\tautoml-obesity-db-1-097f8c93764e4c6fbb270b8cceff7ceb61bc5bd0c61-aws-processing-job\n",
      "\n",
      "automl-obesityvSlg1ey6faHtNtWQEk-007-88d32284-aws-trial\n",
      "\tTrialComponentNames:\n",
      "\tautoml-obesityvSlg1ey6faHtNtWQEk-007-88d32284-aws-training-job\n",
      "\tautoml-obesity-dpp5-csv-1-ffc3275ddf864ac9abf294fd89ceb2502d88c-aws-transform-job\n",
      "\tautoml-obesity-dpp5-1-52cb46ecbdd040049e1c5ff0c2a7bd3f6d4191a13-aws-training-job\n",
      "\tautoml-obesity-db-1-097f8c93764e4c6fbb270b8cceff7ceb61bc5bd0c61-aws-processing-job\n",
      "\n",
      "automl-obesityvSlg1ey6faHtNtWQEk-001-325034c8-aws-trial\n",
      "\tTrialComponentNames:\n",
      "\tautoml-obesityvSlg1ey6faHtNtWQEk-001-325034c8-aws-training-job\n",
      "\tautoml-obesity-dpp4-csv-1-3c0b0b77a00d4df0873c65ee3b3bb9526ff20-aws-transform-job\n",
      "\tautoml-obesity-dpp4-1-41b565ae5774403ca55b2ef75daab98ab94dd13b0-aws-training-job\n",
      "\tautoml-obesity-db-1-097f8c93764e4c6fbb270b8cceff7ceb61bc5bd0c61-aws-processing-job\n",
      "\n",
      "automl-obesityvSlg1ey6faHtNtWQEk-004-7a5bb9be-aws-trial\n",
      "\tTrialComponentNames:\n",
      "\tautoml-obesityvSlg1ey6faHtNtWQEk-004-7a5bb9be-aws-training-job\n",
      "\tautoml-obesity-dpp5-csv-1-ffc3275ddf864ac9abf294fd89ceb2502d88c-aws-transform-job\n",
      "\tautoml-obesity-dpp5-1-52cb46ecbdd040049e1c5ff0c2a7bd3f6d4191a13-aws-training-job\n",
      "\tautoml-obesity-db-1-097f8c93764e4c6fbb270b8cceff7ceb61bc5bd0c61-aws-processing-job\n",
      "\n",
      "automl-obesityvSlg1ey6faHtNtWQEk-002-b0fc29b6-aws-trial\n",
      "\tTrialComponentNames:\n",
      "\tautoml-obesityvSlg1ey6faHtNtWQEk-002-b0fc29b6-aws-training-job\n",
      "\tautoml-obesity-dpp2-csv-1-4c1b0cc2291e434aa325268feafc97ac507e9-aws-transform-job\n",
      "\tautoml-obesity-dpp2-1-9a36998be71f446dbd7ea32bcb055e99430bda923-aws-training-job\n",
      "\tautoml-obesity-db-1-097f8c93764e4c6fbb270b8cceff7ceb61bc5bd0c61-aws-processing-job\n",
      "\n",
      "automl-obesityvSlg1ey6faHtNtWQEk-009-1e8e771c-aws-trial\n",
      "\tTrialComponentNames:\n",
      "\tautoml-obesityvSlg1ey6faHtNtWQEk-009-1e8e771c-aws-training-job\n",
      "\tautoml-obesity-dpp3-rpb-1-5b305988dbc2455aa5027f5ff29846d891139-aws-transform-job\n",
      "\tautoml-obesity-dpp3-1-b8bbed9967e84b009b6d60d429a0ee6ffbd59c91c-aws-training-job\n",
      "\tautoml-obesity-db-1-097f8c93764e4c6fbb270b8cceff7ceb61bc5bd0c61-aws-processing-job\n",
      "\n",
      "automl-obesityvSlg1ey6faHtNtWQEk-008-26f090ee-aws-trial\n",
      "\tTrialComponentNames:\n",
      "\tautoml-obesityvSlg1ey6faHtNtWQEk-008-26f090ee-aws-training-job\n",
      "\tautoml-obesity-dpp2-csv-1-4c1b0cc2291e434aa325268feafc97ac507e9-aws-transform-job\n",
      "\tautoml-obesity-dpp2-1-9a36998be71f446dbd7ea32bcb055e99430bda923-aws-training-job\n",
      "\tautoml-obesity-db-1-097f8c93764e4c6fbb270b8cceff7ceb61bc5bd0c61-aws-processing-job\n",
      "\n",
      "automl-obesityvSlg1ey6faHtNtWQEk-010-9dee109e-aws-trial\n",
      "\tTrialComponentNames:\n",
      "\tautoml-obesityvSlg1ey6faHtNtWQEk-010-9dee109e-aws-training-job\n",
      "\tautoml-obesity-dpp3-rpb-1-5b305988dbc2455aa5027f5ff29846d891139-aws-transform-job\n",
      "\tautoml-obesity-dpp3-1-b8bbed9967e84b009b6d60d429a0ee6ffbd59c91c-aws-training-job\n",
      "\tautoml-obesity-db-1-097f8c93764e4c6fbb270b8cceff7ceb61bc5bd0c61-aws-processing-job\n",
      "\n",
      "Experiment automl-obesity-aws-auto-ml-job deleted\n",
      "automl-obesity:\n",
      "\n",
      "\tautoml-obesityvSlg1ey6faHtNtWQEk-008-26f090ee-model\n",
      "\tautoml-obesity-dpp5-model-59775af9c0d4416bbb6901ae86c98f71729ad\n",
      "\tautoml-obesity-dpp4-model-e5bbc0e1efbe45f5829c58968b65d573556dc\n",
      "\tautoml-obesity-dpp1-model-3b622744d93c49f1bf972a7f144d0df55cc95\n",
      "\tautoml-obesity-dpp3-model-feab32a617a7468f9ebb09ebc647ddcbdf73c\n",
      "\tautoml-obesity-dpp0-model-0584edc6b5f54e1c80f616c43b414c4467a69\n",
      "\tautoml-obesity-dpp6-model-4f22f558b91643f9b09087d0a9c6caf9c8e33\n",
      "\tautoml-obesity-dpp2-model-206030d4a8ce47f0a60b9913b8871607330b0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def cleanup_experiment_resources(experiment_name):\n",
    "    trials = sm.list_trials(ExperimentName=experiment_name)[\"TrialSummaries\"]\n",
    "    print(\"TrialNames:\")\n",
    "    for trial in trials:\n",
    "        trial_name = trial[\"TrialName\"]\n",
    "        print(f\"\\n{trial_name}\")\n",
    "\n",
    "        components_in_trial = sm.list_trial_components(TrialName=trial_name)\n",
    "        print(\"\\tTrialComponentNames:\")\n",
    "        for component in components_in_trial[\"TrialComponentSummaries\"]:\n",
    "            component_name = component[\"TrialComponentName\"]\n",
    "            print(f\"\\t{component_name}\")\n",
    "            sm.disassociate_trial_component(TrialComponentName=component_name, TrialName=trial_name)\n",
    "            try:\n",
    "                # comment out to keep trial components\n",
    "                sm.delete_trial_component(TrialComponentName=component_name)\n",
    "            except:\n",
    "                # component is associated with another trial\n",
    "                continue\n",
    "            # to prevent throttling\n",
    "            time.sleep(5)\n",
    "        sm.delete_trial(TrialName=trial_name)\n",
    "    sm.delete_experiment(ExperimentName=experiment_name)\n",
    "    print(f\"\\nExperiment {experiment_name} deleted\")\n",
    "\n",
    "\n",
    "def cleanup_autopilot_models(autopilot_job_name):\n",
    "    print(\"{0}:\\n\".format(autopilot_job_name))\n",
    "    response = sm.list_models(NameContains=autopilot_job_name)\n",
    "\n",
    "    for model in response[\"Models\"]:\n",
    "        model_name = model[\"ModelName\"]\n",
    "        print(f\"\\t{model_name}\")\n",
    "        sm.delete_model(ModelName=model_name)\n",
    "        # to prevent throttling\n",
    "        time.sleep(3)\n",
    "        \n",
    "cleanup_experiment_resources(\"{0}-aws-auto-ml-job\".format(auto_ml_job_name))\n",
    "cleanup_autopilot_models(auto_ml_job_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef15eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.delete_endpoint(EndpointName=ep_name) \n",
    "sm.delete_endpoint_config(EndpointConfigName=epc_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
