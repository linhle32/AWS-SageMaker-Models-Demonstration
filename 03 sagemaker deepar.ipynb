{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6639a92-7234-4a5e-9d7b-f18186f5ff69",
   "metadata": {},
   "source": [
    "# Forecasting with AWS SageMaker DeepAR\n",
    "\n",
    "In this notebook, we will learn about the **DeepAR** model on AWS SageMaker for the forecasting task.\n",
    "\n",
    "Like Linear Learner, DeepAR is a SageMaker closed model, so there are not much information available on its actual architectures. However, at a high level, DeepAR is a variant of a Recurrent Neural Network (RNN). DeepAR has advantages over `sklearn` models in\n",
    "- Less hyperparameters that directly relate to model architectures\n",
    "- Less manual data processing\n",
    "- Can forecast multiple timeseries simultaneously. This is possible with `sklearn` too, but need more complicated codes\n",
    "- Can forecast further than one time point ahead. This is possible with `sklearn` too, but need more complicated codes\n",
    "- Can forecast a \"confident range\" for each future time point instead of a single value, i.e., where the true value will likely be, for example, next day open price can be from 319 to 322\n",
    "\n",
    "However, DeepAR has its own quirks in that its input and output data must be JSON, and therefore needs quite a bit of data formating. Fortunately, the AWS tutorial is very replicatable on different data, and there are some parts that can be used as-is.\n",
    "\n",
    "## Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a09c517",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/fsspec/registry.py:272: UserWarning: Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n",
      "  warnings.warn(s3_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-11-01</td>\n",
       "      <td>271.600006</td>\n",
       "      <td>273.730011</td>\n",
       "      <td>270.380005</td>\n",
       "      <td>273.510010</td>\n",
       "      <td>250.594238</td>\n",
       "      <td>99495000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-11-02</td>\n",
       "      <td>274.750000</td>\n",
       "      <td>275.230011</td>\n",
       "      <td>269.589996</td>\n",
       "      <td>271.890015</td>\n",
       "      <td>249.109940</td>\n",
       "      <td>122634100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-11-05</td>\n",
       "      <td>272.440002</td>\n",
       "      <td>274.010010</td>\n",
       "      <td>271.350006</td>\n",
       "      <td>273.390015</td>\n",
       "      <td>250.484344</td>\n",
       "      <td>65622500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date        Open        High         Low       Close   Adj Close  \\\n",
       "0  2018-11-01  271.600006  273.730011  270.380005  273.510010  250.594238   \n",
       "1  2018-11-02  274.750000  275.230011  269.589996  271.890015  249.109940   \n",
       "2  2018-11-05  272.440002  274.010010  271.350006  273.390015  250.484344   \n",
       "\n",
       "      Volume  \n",
       "0   99495000  \n",
       "1  122634100  \n",
       "2   65622500  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_name = 'SPY.csv'\n",
    "data_location = 'your bucket'\n",
    "data = pd.read_csv(data_location + data_name)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7bc288-30ea-4aef-9ea9-4aa6af5245df",
   "metadata": {},
   "source": [
    "We need to set `Date` to be `pandas` data-time stamp, and change it to index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d66371ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-11-01</th>\n",
       "      <td>271.600006</td>\n",
       "      <td>273.730011</td>\n",
       "      <td>270.380005</td>\n",
       "      <td>273.510010</td>\n",
       "      <td>250.594238</td>\n",
       "      <td>99495000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-02</th>\n",
       "      <td>274.750000</td>\n",
       "      <td>275.230011</td>\n",
       "      <td>269.589996</td>\n",
       "      <td>271.890015</td>\n",
       "      <td>249.109940</td>\n",
       "      <td>122634100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-05</th>\n",
       "      <td>272.440002</td>\n",
       "      <td>274.010010</td>\n",
       "      <td>271.350006</td>\n",
       "      <td>273.390015</td>\n",
       "      <td>250.484344</td>\n",
       "      <td>65622500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "2018-11-01  271.600006  273.730011  270.380005  273.510010  250.594238   \n",
       "2018-11-02  274.750000  275.230011  269.589996  271.890015  249.109940   \n",
       "2018-11-05  272.440002  274.010010  271.350006  273.390015  250.484344   \n",
       "\n",
       "               Volume  \n",
       "2018-11-01   99495000  \n",
       "2018-11-02  122634100  \n",
       "2018-11-05   65622500  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "data.index.name = None\n",
    "data.columns = data.columns.get_level_values(0)\n",
    "\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad80edec-d122-4824-8e75-a81a73a2d057",
   "metadata": {},
   "source": [
    "Similar to `sklearn`, we split data by a time point. We also need the timestamp that the data starts and ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f9c9e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dataset = pd.to_datetime('2018-01-01')\n",
    "end_training = pd.to_datetime('2022-12-31')\n",
    "end_dataset = pd.to_datetime('2023-12-31')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df56881c-3301-484d-bdf3-b7494cb8d40b",
   "metadata": {},
   "source": [
    "Then, set up some hyperparameters\n",
    "- `prediction_length`: how many time points to forecast ahead, e.g., 5 means to predict the next 5 days from a current day\n",
    "- `context_length`: how many RNN hidden states to go back. This is similar to the `window` parameter in `sklearn` models, but is different in how it is computed. `context_length = 15` will be able to get older historical information than `window = 15`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1a4e0c-669c-4887-b42a-12c2a63e0774",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_length = 5\n",
    "context_length = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671115cf-28e7-4202-9d64-f647b396ecdb",
   "metadata": {},
   "source": [
    "Now, we prepare the training and testing data in the format required by DeepAR. You can read more about this format in https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a00e8ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "freq = '1D'\n",
    "training_data = [\n",
    "    {\n",
    "        'start': str(start_dataset),\n",
    "        'target': data[ts][start_dataset : end_training].tolist(),\n",
    "    }\n",
    "    for ts in data\n",
    "]\n",
    "#we will perform testing on 4 different forecasting periods, from 1 business week to 4 business weeks\n",
    "num_test_windows = 4\n",
    "test_data = [\n",
    "    {\n",
    "        'start': str(start_dataset),\n",
    "        'target': data[ts][start_dataset : end_training + timedelta(days=k * prediction_length)].tolist(),\n",
    "    }\n",
    "    for k in range(1, num_test_windows + 1)\n",
    "    for ts in data\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17e7046-cfa8-46ea-8b82-93e6b4420523",
   "metadata": {},
   "source": [
    "Finally, we write the data as JSON files and upload to our S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81bd9b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, 'wb') as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode('utf-8'))\n",
    "            fp.write('\\n'.encode('utf-8'))\n",
    "write_dicts_to_file('train.json', training_data)\n",
    "write_dicts_to_file('test.json', test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c6de9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "sess = sagemaker.Session()                        # get our current SageMaker session\n",
    "bucket = 'lle13-it7143'                           # this should be the name of the bucket we created in module 9\n",
    "prefix = 'forecasting'                            # the folder to store your data in the S3 instance\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.json')).upload_file('train.json')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'test/test.json')).upload_file('test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c4aa4a-be59-4e4e-af11-be8b63791626",
   "metadata": {},
   "source": [
    "### Build and Train Model\n",
    "\n",
    "Now, we download the model image then create and train the model. Unlike XGBoost and Linear Learner, DeepAR does not have hyperparameters that change the its architecture, so we won't finetune this model. The only values that can be tuned in the dictionary `hyperparameters` are `epochs`, `early_stopping_patience`, `mini_batch_size`, and `learning_rate`, but these four should not drastically impact the model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a83cb4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: 1.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "WARNING:sagemaker.deprecations:train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "WARNING:sagemaker.deprecations:train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "INFO:sagemaker:Creating training-job with name: deepar-forecast-2024-01-05-02-28-59-968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-05 02:29:00 Starting - Starting the training job...\n",
      "2024-01-05 02:29:24 Starting - Preparing the instances for training.........\n",
      "2024-01-05 02:30:37 Downloading - Downloading input data...\n",
      "2024-01-05 02:31:02 Downloading - Downloading the training image.....................\n",
      "2024-01-05 02:34:33 Training - Training image download completed. Training in progress..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34mRunning custom environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/mxnet/model.py:97: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if num_device is 1 and 'dist' not in kvstore:\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:34:59 INFO 140362387572544] Reading default configuration from /opt/amazon/lib/python3.8/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:34:59 INFO 140362387572544] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'context_length': '15', 'early_stopping_patience': '40', 'epochs': '100', 'learning_rate': '1E-3', 'mini_batch_size': '32', 'prediction_length': '5', 'time_freq': '1D'}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:34:59 INFO 140362387572544] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '40', 'embedding_dimension': '10', 'learning_rate': '1E-3', 'likelihood': 'student-t', 'mini_batch_size': '32', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'context_length': '15', 'epochs': '100', 'prediction_length': '5', 'time_freq': '1D'}\u001b[0m\n",
      "\u001b[34mProcess 7 is a worker.\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:34:59 INFO 140362387572544] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] random_seed is None\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] Training set statistics:\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] Real time series\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] number of time series: 6\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] number of observations: 6288\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] mean target length: 1048.0\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] min/mean/max target: 210.5848846435547/14400187.495964536/392220704.0\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] mean abs(target): 14400187.495964536\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] contains missing values: no\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] Small number of time series. Doing 54 passes over dataset with prob 0.9876543209876544 per epoch.\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] Test set statistics:\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] Real time series\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] number of time series: 24\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] number of observations: 25338\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] mean target length: 1055.75\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] min/mean/max target: 210.5848846435547/14391905.918239156/392220704.0\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] mean abs(target): 14391905.918239156\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] contains missing values: no\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/algorithm/core/date_feature_set.py:44: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return index.weekofyear / 51.0 - 0.5\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] #memory_usage::<batchbuffer> = 7.3095703125 mb\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.8/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] nvidia-smi: took 0.033 seconds to run.\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422100.1072037, \"EndTime\": 1704422100.1777048, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 69.33045387268066, \"count\": 1, \"min\": 69.33045387268066, \"max\": 69.33045387268066}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] #memory_usage::<model> = 3 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422100.1777985, \"EndTime\": 1704422100.2686365, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 161.2839698791504, \"count\": 1, \"min\": 161.2839698791504, \"max\": 161.2839698791504}}}\u001b[0m\n",
      "\u001b[34m[02:35:00] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_Cuda_11.1.x.398.0/AL2_x86_64/generic-flavor/src/src/operator/nn/mkldnn/mkldnn_base.cc:74: Allocate 5120 bytes with malloc directly\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] Epoch[0] Batch[0] avg_epoch_loss=9.259766\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=9.259765625\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] Epoch[0] Batch[5] avg_epoch_loss=8.157289\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=8.157288948694864\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:00 INFO 140362387572544] Epoch[0] Batch [5]#011Speed: 536.90 samples/sec#011loss=8.157289\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:01 INFO 140362387572544] Epoch[0] Batch[10] avg_epoch_loss=8.035105\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:01 INFO 140362387572544] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=7.888484001159668\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:01 INFO 140362387572544] Epoch[0] Batch [10]#011Speed: 559.56 samples/sec#011loss=7.888484\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:01 INFO 140362387572544] processed a total of 332 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422100.2687097, \"EndTime\": 1704422101.1073143, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 100.0, \"count\": 1, \"min\": 100, \"max\": 100}, \"update.time\": {\"sum\": 838.5012149810791, \"count\": 1, \"min\": 838.5012149810791, \"max\": 838.5012149810791}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:01 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=395.87331452864225 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:01 INFO 140362387572544] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:01 INFO 140362387572544] #quality_metric: host=algo-1, epoch=0, train loss <loss>=8.035104881633412\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:01 INFO 140362387572544] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:01 INFO 140362387572544] Saved checkpoint to \"/opt/ml/model/state_7e4e1f1c-86e7-4011-9b93-f808d831cab2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422101.10742, \"EndTime\": 1704422101.1202717, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.0697021484375, \"count\": 1, \"min\": 12.0697021484375, \"max\": 12.0697021484375}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:01 INFO 140362387572544] Epoch[1] Batch[0] avg_epoch_loss=7.954974\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:01 INFO 140362387572544] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=7.9549736976623535\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:01 INFO 140362387572544] Epoch[1] Batch[5] avg_epoch_loss=7.528054\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:01 INFO 140362387572544] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=7.528054475784302\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:01 INFO 140362387572544] Epoch[1] Batch [5]#011Speed: 559.69 samples/sec#011loss=7.528054\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:02 INFO 140362387572544] Epoch[1] Batch[10] avg_epoch_loss=6.942633\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:02 INFO 140362387572544] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=6.240126323699951\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:02 INFO 140362387572544] Epoch[1] Batch [10]#011Speed: 382.54 samples/sec#011loss=6.240126\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:02 INFO 140362387572544] processed a total of 326 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422101.1203413, \"EndTime\": 1704422102.0221527, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 901.7548561096191, \"count\": 1, \"min\": 901.7548561096191, \"max\": 901.7548561096191}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:02 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=361.44163534694184 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:02 INFO 140362387572544] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:02 INFO 140362387572544] #quality_metric: host=algo-1, epoch=1, train loss <loss>=6.942632588473233\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:02 INFO 140362387572544] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:02 INFO 140362387572544] Saved checkpoint to \"/opt/ml/model/state_cebb9595-ee3b-4a2b-aa71-70eb7dddddc2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422102.0223036, \"EndTime\": 1704422102.0353358, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.41922378540039, \"count\": 1, \"min\": 12.41922378540039, \"max\": 12.41922378540039}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:02 INFO 140362387572544] Epoch[2] Batch[0] avg_epoch_loss=6.248740\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:02 INFO 140362387572544] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=6.248739719390869\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:02 INFO 140362387572544] Epoch[2] Batch[5] avg_epoch_loss=6.741443\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:02 INFO 140362387572544] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=6.741443475087483\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:02 INFO 140362387572544] Epoch[2] Batch [5]#011Speed: 495.72 samples/sec#011loss=6.741443\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:02 INFO 140362387572544] processed a total of 311 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422102.035415, \"EndTime\": 1704422102.8717408, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 836.2596035003662, \"count\": 1, \"min\": 836.2596035003662, \"max\": 836.2596035003662}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:02 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=371.832175207683 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:02 INFO 140362387572544] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:02 INFO 140362387572544] #quality_metric: host=algo-1, epoch=2, train loss <loss>=6.829828214645386\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:02 INFO 140362387572544] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:02 INFO 140362387572544] Saved checkpoint to \"/opt/ml/model/state_8a69ab32-2324-4ae0-99ad-e77321b9d29f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422102.871839, \"EndTime\": 1704422102.885156, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.042045593261719, \"count\": 1, \"min\": 12.042045593261719, \"max\": 12.042045593261719}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:03 INFO 140362387572544] Epoch[3] Batch[0] avg_epoch_loss=6.138680\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:03 INFO 140362387572544] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=6.1386799812316895\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:03 INFO 140362387572544] Epoch[3] Batch[5] avg_epoch_loss=6.607782\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:03 INFO 140362387572544] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=6.607781966527303\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:03 INFO 140362387572544] Epoch[3] Batch [5]#011Speed: 329.83 samples/sec#011loss=6.607782\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:03 INFO 140362387572544] processed a total of 310 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422102.885247, \"EndTime\": 1704422103.960411, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1075.0300884246826, \"count\": 1, \"min\": 1075.0300884246826, \"max\": 1075.0300884246826}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:03 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=288.30960824895885 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:03 INFO 140362387572544] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:03 INFO 140362387572544] #quality_metric: host=algo-1, epoch=3, train loss <loss>=6.428512907028198\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:03 INFO 140362387572544] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:03 INFO 140362387572544] Saved checkpoint to \"/opt/ml/model/state_b626d39d-83ab-48e6-b2f4-100b5f78218a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422103.9605706, \"EndTime\": 1704422103.9795067, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 18.120765686035156, \"count\": 1, \"min\": 18.120765686035156, \"max\": 18.120765686035156}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:04 INFO 140362387572544] Epoch[4] Batch[0] avg_epoch_loss=5.859145\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:04 INFO 140362387572544] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=5.859144687652588\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:04 INFO 140362387572544] Epoch[4] Batch[5] avg_epoch_loss=5.918942\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:04 INFO 140362387572544] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=5.918942372004191\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:04 INFO 140362387572544] Epoch[4] Batch [5]#011Speed: 361.25 samples/sec#011loss=5.918942\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:05 INFO 140362387572544] Epoch[4] Batch[10] avg_epoch_loss=6.266171\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:05 INFO 140362387572544] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=6.682844352722168\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:05 INFO 140362387572544] Epoch[4] Batch [10]#011Speed: 389.05 samples/sec#011loss=6.682844\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:05 INFO 140362387572544] processed a total of 352 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422103.9796884, \"EndTime\": 1704422105.1562476, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1176.478385925293, \"count\": 1, \"min\": 1176.478385925293, \"max\": 1176.478385925293}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:05 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=299.1593375353029 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:05 INFO 140362387572544] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:05 INFO 140362387572544] #quality_metric: host=algo-1, epoch=4, train loss <loss>=6.266170545057817\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:05 INFO 140362387572544] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:05 INFO 140362387572544] Saved checkpoint to \"/opt/ml/model/state_b884494f-1123-417f-9eb1-e60f03b16331-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422105.1563547, \"EndTime\": 1704422105.1768334, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 19.386768341064453, \"count\": 1, \"min\": 19.386768341064453, \"max\": 19.386768341064453}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:05 INFO 140362387572544] Epoch[5] Batch[0] avg_epoch_loss=6.414495\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:05 INFO 140362387572544] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=6.414494514465332\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:05 INFO 140362387572544] Epoch[5] Batch[5] avg_epoch_loss=6.697807\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:05 INFO 140362387572544] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=6.697807153065999\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:05 INFO 140362387572544] Epoch[5] Batch [5]#011Speed: 397.33 samples/sec#011loss=6.697807\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:06 INFO 140362387572544] Epoch[5] Batch[10] avg_epoch_loss=6.804765\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:06 INFO 140362387572544] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=6.9331135749816895\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:06 INFO 140362387572544] Epoch[5] Batch [10]#011Speed: 368.40 samples/sec#011loss=6.933114\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:06 INFO 140362387572544] processed a total of 350 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422105.177307, \"EndTime\": 1704422106.282533, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1105.1530838012695, \"count\": 1, \"min\": 1105.1530838012695, \"max\": 1105.1530838012695}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:06 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=316.65134466226954 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:06 INFO 140362387572544] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:06 INFO 140362387572544] #quality_metric: host=algo-1, epoch=5, train loss <loss>=6.804764617573131\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:06 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:06 INFO 140362387572544] Epoch[6] Batch[0] avg_epoch_loss=6.333181\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:06 INFO 140362387572544] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=6.333181381225586\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:06 INFO 140362387572544] Epoch[6] Batch[5] avg_epoch_loss=6.469144\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:06 INFO 140362387572544] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=6.469144105911255\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:06 INFO 140362387572544] Epoch[6] Batch [5]#011Speed: 528.69 samples/sec#011loss=6.469144\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:07 INFO 140362387572544] processed a total of 287 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422106.2826545, \"EndTime\": 1704422107.0553136, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 771.4061737060547, \"count\": 1, \"min\": 771.4061737060547, \"max\": 771.4061737060547}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:07 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=371.8455687381227 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:07 INFO 140362387572544] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:07 INFO 140362387572544] #quality_metric: host=algo-1, epoch=6, train loss <loss>=6.149560239579943\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:07 INFO 140362387572544] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:07 INFO 140362387572544] Saved checkpoint to \"/opt/ml/model/state_4493c0e2-0c9f-4f8f-88a5-35656abd7ba8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422107.0555105, \"EndTime\": 1704422107.0692787, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.677192687988281, \"count\": 1, \"min\": 12.677192687988281, \"max\": 12.677192687988281}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:07 INFO 140362387572544] Epoch[7] Batch[0] avg_epoch_loss=5.914728\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:07 INFO 140362387572544] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=5.914728164672852\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:07 INFO 140362387572544] Epoch[7] Batch[5] avg_epoch_loss=5.840647\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:07 INFO 140362387572544] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=5.840646982192993\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:07 INFO 140362387572544] Epoch[7] Batch [5]#011Speed: 527.54 samples/sec#011loss=5.840647\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:07 INFO 140362387572544] Epoch[7] Batch[10] avg_epoch_loss=6.063656\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:07 INFO 140362387572544] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=6.3312668800354\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:07 INFO 140362387572544] Epoch[7] Batch [10]#011Speed: 555.32 samples/sec#011loss=6.331267\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:07 INFO 140362387572544] processed a total of 327 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422107.0693545, \"EndTime\": 1704422107.8522549, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 782.8497886657715, \"count\": 1, \"min\": 782.8497886657715, \"max\": 782.8497886657715}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:07 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=417.6448723804652 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:07 INFO 140362387572544] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:07 INFO 140362387572544] #quality_metric: host=algo-1, epoch=7, train loss <loss>=6.0636560266668145\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:07 INFO 140362387572544] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:07 INFO 140362387572544] Saved checkpoint to \"/opt/ml/model/state_d33e7f2f-ba3e-4b6a-b96f-e9e409ab73e6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422107.852333, \"EndTime\": 1704422107.864992, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.219905853271484, \"count\": 1, \"min\": 12.219905853271484, \"max\": 12.219905853271484}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:08 INFO 140362387572544] Epoch[8] Batch[0] avg_epoch_loss=6.966710\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:08 INFO 140362387572544] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=6.966710090637207\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:08 INFO 140362387572544] Epoch[8] Batch[5] avg_epoch_loss=6.520130\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:08 INFO 140362387572544] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=6.520130157470703\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:08 INFO 140362387572544] Epoch[8] Batch [5]#011Speed: 592.30 samples/sec#011loss=6.520130\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:08 INFO 140362387572544] Epoch[8] Batch[10] avg_epoch_loss=6.140716\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:08 INFO 140362387572544] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=5.685418605804443\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:08 INFO 140362387572544] Epoch[8] Batch [10]#011Speed: 580.68 samples/sec#011loss=5.685419\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:08 INFO 140362387572544] processed a total of 323 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422107.86505, \"EndTime\": 1704422108.601454, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 736.342191696167, \"count\": 1, \"min\": 736.342191696167, \"max\": 736.342191696167}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:08 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=438.5819001630324 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:08 INFO 140362387572544] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:08 INFO 140362387572544] #quality_metric: host=algo-1, epoch=8, train loss <loss>=6.140715815804222\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:08 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:08 INFO 140362387572544] Epoch[9] Batch[0] avg_epoch_loss=5.102294\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:08 INFO 140362387572544] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=5.102294445037842\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:09 INFO 140362387572544] Epoch[9] Batch[5] avg_epoch_loss=5.918956\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:09 INFO 140362387572544] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=5.918956120808919\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:09 INFO 140362387572544] Epoch[9] Batch [5]#011Speed: 606.40 samples/sec#011loss=5.918956\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:09 INFO 140362387572544] Epoch[9] Batch[10] avg_epoch_loss=6.049131\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:09 INFO 140362387572544] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=6.205340480804443\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:09 INFO 140362387572544] Epoch[9] Batch [10]#011Speed: 579.93 samples/sec#011loss=6.205340\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:09 INFO 140362387572544] processed a total of 332 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422108.6015403, \"EndTime\": 1704422109.3372223, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 735.2211475372314, \"count\": 1, \"min\": 735.2211475372314, \"max\": 735.2211475372314}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:09 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=451.492632530476 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:09 INFO 140362387572544] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:09 INFO 140362387572544] #quality_metric: host=algo-1, epoch=9, train loss <loss>=6.049130829897794\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:09 INFO 140362387572544] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:09 INFO 140362387572544] Saved checkpoint to \"/opt/ml/model/state_30633a7c-a785-4de9-a77c-7bd49b908da8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422109.337303, \"EndTime\": 1704422109.349746, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 11.988639831542969, \"count\": 1, \"min\": 11.988639831542969, \"max\": 11.988639831542969}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:09 INFO 140362387572544] Epoch[10] Batch[0] avg_epoch_loss=6.204349\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:09 INFO 140362387572544] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=6.204348564147949\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:09 INFO 140362387572544] Epoch[10] Batch[5] avg_epoch_loss=5.948213\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:09 INFO 140362387572544] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=5.948212544123332\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:09 INFO 140362387572544] Epoch[10] Batch [5]#011Speed: 585.47 samples/sec#011loss=5.948213\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:10 INFO 140362387572544] Epoch[10] Batch[10] avg_epoch_loss=5.894581\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:10 INFO 140362387572544] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=5.830223274230957\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:10 INFO 140362387572544] Epoch[10] Batch [10]#011Speed: 572.40 samples/sec#011loss=5.830223\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:10 INFO 140362387572544] processed a total of 332 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422109.3498154, \"EndTime\": 1704422110.0963013, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 746.4220523834229, \"count\": 1, \"min\": 746.4220523834229, \"max\": 746.4220523834229}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:10 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=444.6949677139153 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:10 INFO 140362387572544] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:10 INFO 140362387572544] #quality_metric: host=algo-1, epoch=10, train loss <loss>=5.894581057808616\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:10 INFO 140362387572544] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:10 INFO 140362387572544] Saved checkpoint to \"/opt/ml/model/state_e4433ad9-9622-43ee-a66b-eb7bedfbefcc-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422110.0964165, \"EndTime\": 1704422110.1089551, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 11.852025985717773, \"count\": 1, \"min\": 11.852025985717773, \"max\": 11.852025985717773}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:10 INFO 140362387572544] Epoch[11] Batch[0] avg_epoch_loss=5.231408\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:10 INFO 140362387572544] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=5.231407642364502\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:10 INFO 140362387572544] Epoch[11] Batch[5] avg_epoch_loss=6.539240\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:10 INFO 140362387572544] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=6.539239962895711\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:10 INFO 140362387572544] Epoch[11] Batch [5]#011Speed: 513.14 samples/sec#011loss=6.539240\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:10 INFO 140362387572544] processed a total of 318 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422110.1090353, \"EndTime\": 1704422110.859846, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 750.744104385376, \"count\": 1, \"min\": 750.744104385376, \"max\": 750.744104385376}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:10 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=423.51277010758105 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:10 INFO 140362387572544] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:10 INFO 140362387572544] #quality_metric: host=algo-1, epoch=11, train loss <loss>=6.237579393386841\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:10 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:11 INFO 140362387572544] Epoch[12] Batch[0] avg_epoch_loss=7.168421\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:11 INFO 140362387572544] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=7.168421268463135\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:11 INFO 140362387572544] Epoch[12] Batch[5] avg_epoch_loss=6.530940\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:11 INFO 140362387572544] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=6.5309403737386065\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:11 INFO 140362387572544] Epoch[12] Batch [5]#011Speed: 602.00 samples/sec#011loss=6.530940\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:11 INFO 140362387572544] processed a total of 307 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422110.859926, \"EndTime\": 1704422111.548945, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 687.8483295440674, \"count\": 1, \"min\": 687.8483295440674, \"max\": 687.8483295440674}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:11 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=446.2333247042817 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:11 INFO 140362387572544] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:11 INFO 140362387572544] #quality_metric: host=algo-1, epoch=12, train loss <loss>=6.589181900024414\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:11 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:11 INFO 140362387572544] Epoch[13] Batch[0] avg_epoch_loss=5.966432\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:11 INFO 140362387572544] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=5.966432094573975\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:12 INFO 140362387572544] Epoch[13] Batch[5] avg_epoch_loss=6.314185\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:12 INFO 140362387572544] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=6.314184506734212\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:12 INFO 140362387572544] Epoch[13] Batch [5]#011Speed: 575.25 samples/sec#011loss=6.314185\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:12 INFO 140362387572544] processed a total of 314 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422111.5490353, \"EndTime\": 1704422112.2596912, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 709.8093032836914, \"count\": 1, \"min\": 709.8093032836914, \"max\": 709.8093032836914}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:12 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=442.2881026878591 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:12 INFO 140362387572544] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:12 INFO 140362387572544] #quality_metric: host=algo-1, epoch=13, train loss <loss>=6.284380006790161\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:12 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:12 INFO 140362387572544] Epoch[14] Batch[0] avg_epoch_loss=6.440956\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:12 INFO 140362387572544] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=6.440956115722656\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:12 INFO 140362387572544] Epoch[14] Batch[5] avg_epoch_loss=5.994781\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:12 INFO 140362387572544] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=5.994781494140625\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:12 INFO 140362387572544] Epoch[14] Batch [5]#011Speed: 581.35 samples/sec#011loss=5.994781\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:13 INFO 140362387572544] Epoch[14] Batch[10] avg_epoch_loss=6.160168\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:13 INFO 140362387572544] #quality_metric: host=algo-1, epoch=14, batch=10 train loss <loss>=6.358630752563476\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:13 INFO 140362387572544] Epoch[14] Batch [10]#011Speed: 551.58 samples/sec#011loss=6.358631\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:13 INFO 140362387572544] processed a total of 349 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422112.2597806, \"EndTime\": 1704422113.019832, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 759.2339515686035, \"count\": 1, \"min\": 759.2339515686035, \"max\": 759.2339515686035}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:13 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=459.5858223435104 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:13 INFO 140362387572544] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:13 INFO 140362387572544] #quality_metric: host=algo-1, epoch=14, train loss <loss>=6.160167520696467\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:13 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:13 INFO 140362387572544] Epoch[15] Batch[0] avg_epoch_loss=6.884864\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:13 INFO 140362387572544] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=6.88486385345459\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:13 INFO 140362387572544] Epoch[15] Batch[5] avg_epoch_loss=6.648204\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:13 INFO 140362387572544] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=6.648204326629639\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:13 INFO 140362387572544] Epoch[15] Batch [5]#011Speed: 594.33 samples/sec#011loss=6.648204\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:13 INFO 140362387572544] processed a total of 304 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422113.0199332, \"EndTime\": 1704422113.7844, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 763.9241218566895, \"count\": 1, \"min\": 763.9241218566895, \"max\": 763.9241218566895}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:13 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=397.69096360496405 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:13 INFO 140362387572544] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:13 INFO 140362387572544] #quality_metric: host=algo-1, epoch=15, train loss <loss>=6.208857488632202\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:13 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:13 INFO 140362387572544] Epoch[16] Batch[0] avg_epoch_loss=7.659628\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:13 INFO 140362387572544] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=7.659627914428711\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:14 INFO 140362387572544] Epoch[16] Batch[5] avg_epoch_loss=6.245621\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:14 INFO 140362387572544] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=6.245620568593343\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:14 INFO 140362387572544] Epoch[16] Batch [5]#011Speed: 598.12 samples/sec#011loss=6.245621\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:14 INFO 140362387572544] processed a total of 309 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422113.7848427, \"EndTime\": 1704422114.4643247, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 678.7364482879639, \"count\": 1, \"min\": 678.7364482879639, \"max\": 678.7364482879639}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:14 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=455.1692910675779 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:14 INFO 140362387572544] #progress_metric: host=algo-1, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:14 INFO 140362387572544] #quality_metric: host=algo-1, epoch=16, train loss <loss>=6.144964599609375\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:14 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:14 INFO 140362387572544] Epoch[17] Batch[0] avg_epoch_loss=8.074978\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:14 INFO 140362387572544] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=8.07497787475586\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:14 INFO 140362387572544] Epoch[17] Batch[5] avg_epoch_loss=6.881869\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:14 INFO 140362387572544] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=6.881869316101074\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:14 INFO 140362387572544] Epoch[17] Batch [5]#011Speed: 507.96 samples/sec#011loss=6.881869\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:15 INFO 140362387572544] processed a total of 272 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422114.4644115, \"EndTime\": 1704422115.1490326, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 683.9537620544434, \"count\": 1, \"min\": 683.9537620544434, \"max\": 683.9537620544434}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:15 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=397.59345644893165 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:15 INFO 140362387572544] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:15 INFO 140362387572544] #quality_metric: host=algo-1, epoch=17, train loss <loss>=7.588982794019911\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:15 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:15 INFO 140362387572544] Epoch[18] Batch[0] avg_epoch_loss=5.117259\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:15 INFO 140362387572544] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=5.117258548736572\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:15 INFO 140362387572544] Epoch[18] Batch[5] avg_epoch_loss=6.218786\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:15 INFO 140362387572544] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=6.218785842259725\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:15 INFO 140362387572544] Epoch[18] Batch [5]#011Speed: 576.83 samples/sec#011loss=6.218786\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:15 INFO 140362387572544] Epoch[18] Batch[10] avg_epoch_loss=6.097559\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:15 INFO 140362387572544] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=5.952086353302002\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:15 INFO 140362387572544] Epoch[18] Batch [10]#011Speed: 525.61 samples/sec#011loss=5.952086\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:15 INFO 140362387572544] processed a total of 330 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422115.1491148, \"EndTime\": 1704422115.9242654, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 774.5373249053955, \"count\": 1, \"min\": 774.5373249053955, \"max\": 774.5373249053955}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:15 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=425.9822654131238 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:15 INFO 140362387572544] #progress_metric: host=algo-1, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:15 INFO 140362387572544] #quality_metric: host=algo-1, epoch=18, train loss <loss>=6.0975588018243965\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:15 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:16 INFO 140362387572544] Epoch[19] Batch[0] avg_epoch_loss=6.372270\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:16 INFO 140362387572544] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=6.372269630432129\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:16 INFO 140362387572544] Epoch[19] Batch[5] avg_epoch_loss=6.013080\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:16 INFO 140362387572544] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=6.013079722722371\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:16 INFO 140362387572544] Epoch[19] Batch [5]#011Speed: 542.05 samples/sec#011loss=6.013080\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:16 INFO 140362387572544] Epoch[19] Batch[10] avg_epoch_loss=5.730177\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:16 INFO 140362387572544] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=5.390694332122803\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:16 INFO 140362387572544] Epoch[19] Batch [10]#011Speed: 566.37 samples/sec#011loss=5.390694\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:16 INFO 140362387572544] processed a total of 334 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422115.9243612, \"EndTime\": 1704422116.6934154, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 768.4328556060791, \"count\": 1, \"min\": 768.4328556060791, \"max\": 768.4328556060791}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:16 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=434.5816871688825 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:16 INFO 140362387572544] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:16 INFO 140362387572544] #quality_metric: host=algo-1, epoch=19, train loss <loss>=5.73017727244984\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:16 INFO 140362387572544] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:16 INFO 140362387572544] Saved checkpoint to \"/opt/ml/model/state_3fc1abc2-d5df-4c0c-bcb7-42805c59a4b4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422116.6934974, \"EndTime\": 1704422116.7059026, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 11.959314346313477, \"count\": 1, \"min\": 11.959314346313477, \"max\": 11.959314346313477}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:16 INFO 140362387572544] Epoch[20] Batch[0] avg_epoch_loss=6.133186\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:16 INFO 140362387572544] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=6.133185863494873\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:17 INFO 140362387572544] Epoch[20] Batch[5] avg_epoch_loss=6.001448\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:17 INFO 140362387572544] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=6.001448154449463\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:17 INFO 140362387572544] Epoch[20] Batch [5]#011Speed: 580.35 samples/sec#011loss=6.001448\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:17 INFO 140362387572544] Epoch[20] Batch[10] avg_epoch_loss=5.861250\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:17 INFO 140362387572544] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=5.693012762069702\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:17 INFO 140362387572544] Epoch[20] Batch [10]#011Speed: 585.70 samples/sec#011loss=5.693013\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:17 INFO 140362387572544] processed a total of 324 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422116.7059743, \"EndTime\": 1704422117.446223, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 740.1900291442871, \"count\": 1, \"min\": 740.1900291442871, \"max\": 740.1900291442871}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:17 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=437.65804718749393 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:17 INFO 140362387572544] #progress_metric: host=algo-1, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:17 INFO 140362387572544] #quality_metric: host=algo-1, epoch=20, train loss <loss>=5.861250248822299\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:17 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:17 INFO 140362387572544] Epoch[21] Batch[0] avg_epoch_loss=4.685768\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:17 INFO 140362387572544] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=4.685768127441406\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:17 INFO 140362387572544] Epoch[21] Batch[5] avg_epoch_loss=6.480256\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:17 INFO 140362387572544] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=6.480256001154582\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:17 INFO 140362387572544] Epoch[21] Batch [5]#011Speed: 532.10 samples/sec#011loss=6.480256\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:18 INFO 140362387572544] Epoch[21] Batch[10] avg_epoch_loss=5.988845\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:18 INFO 140362387572544] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=5.399150753021241\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:18 INFO 140362387572544] Epoch[21] Batch [10]#011Speed: 556.51 samples/sec#011loss=5.399151\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:18 INFO 140362387572544] processed a total of 326 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422117.4463024, \"EndTime\": 1704422118.2281585, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 781.4133167266846, \"count\": 1, \"min\": 781.4133167266846, \"max\": 781.4133167266846}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:18 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=417.1184633392403 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:18 INFO 140362387572544] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:18 INFO 140362387572544] #quality_metric: host=algo-1, epoch=21, train loss <loss>=5.988844524730336\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:18 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:18 INFO 140362387572544] Epoch[22] Batch[0] avg_epoch_loss=6.108609\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:18 INFO 140362387572544] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=6.108608722686768\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:18 INFO 140362387572544] Epoch[22] Batch[5] avg_epoch_loss=6.609716\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:18 INFO 140362387572544] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=6.609716176986694\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:18 INFO 140362387572544] Epoch[22] Batch [5]#011Speed: 598.53 samples/sec#011loss=6.609716\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:18 INFO 140362387572544] processed a total of 297 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422118.228253, \"EndTime\": 1704422118.9226437, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 693.8638687133789, \"count\": 1, \"min\": 693.8638687133789, \"max\": 693.8638687133789}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:18 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=427.96065000664765 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:18 INFO 140362387572544] #progress_metric: host=algo-1, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:18 INFO 140362387572544] #quality_metric: host=algo-1, epoch=22, train loss <loss>=6.282536411285401\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:18 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:19 INFO 140362387572544] Epoch[23] Batch[0] avg_epoch_loss=5.142015\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:19 INFO 140362387572544] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=5.142014503479004\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:19 INFO 140362387572544] Epoch[23] Batch[5] avg_epoch_loss=5.861147\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:19 INFO 140362387572544] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=5.861146608988444\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:19 INFO 140362387572544] Epoch[23] Batch [5]#011Speed: 606.86 samples/sec#011loss=5.861147\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:19 INFO 140362387572544] processed a total of 315 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422118.9227264, \"EndTime\": 1704422119.5954757, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 672.2755432128906, \"count\": 1, \"min\": 672.2755432128906, \"max\": 672.2755432128906}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:19 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=468.4664986926821 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:19 INFO 140362387572544] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:19 INFO 140362387572544] #quality_metric: host=algo-1, epoch=23, train loss <loss>=5.9039712905883786\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:19 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:19 INFO 140362387572544] Epoch[24] Batch[0] avg_epoch_loss=7.310826\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:19 INFO 140362387572544] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=7.310826301574707\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:20 INFO 140362387572544] Epoch[24] Batch[5] avg_epoch_loss=6.398133\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:20 INFO 140362387572544] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=6.398132721583049\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:20 INFO 140362387572544] Epoch[24] Batch [5]#011Speed: 591.55 samples/sec#011loss=6.398133\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:20 INFO 140362387572544] processed a total of 307 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422119.5955656, \"EndTime\": 1704422120.2791116, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 683.0751895904541, \"count\": 1, \"min\": 683.0751895904541, \"max\": 683.0751895904541}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:20 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=449.3428772431352 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:20 INFO 140362387572544] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:20 INFO 140362387572544] #quality_metric: host=algo-1, epoch=24, train loss <loss>=5.929552769660949\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:20 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:20 INFO 140362387572544] Epoch[25] Batch[0] avg_epoch_loss=6.833673\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:20 INFO 140362387572544] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=6.833673000335693\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:20 INFO 140362387572544] Epoch[25] Batch[5] avg_epoch_loss=6.747456\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:20 INFO 140362387572544] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=6.747456153233846\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:20 INFO 140362387572544] Epoch[25] Batch [5]#011Speed: 593.23 samples/sec#011loss=6.747456\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:20 INFO 140362387572544] processed a total of 300 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422120.2792125, \"EndTime\": 1704422121.0003464, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 720.5243110656738, \"count\": 1, \"min\": 720.5243110656738, \"max\": 720.5243110656738}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:21 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=416.2914238999861 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:21 INFO 140362387572544] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:21 INFO 140362387572544] #quality_metric: host=algo-1, epoch=25, train loss <loss>=6.217047452926636\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:21 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:21 INFO 140362387572544] Epoch[26] Batch[0] avg_epoch_loss=7.927722\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:21 INFO 140362387572544] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=7.9277215003967285\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:21 INFO 140362387572544] Epoch[26] Batch[5] avg_epoch_loss=7.056134\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:21 INFO 140362387572544] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=7.056134064992269\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:21 INFO 140362387572544] Epoch[26] Batch [5]#011Speed: 583.15 samples/sec#011loss=7.056134\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:21 INFO 140362387572544] Epoch[26] Batch[10] avg_epoch_loss=6.589879\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:21 INFO 140362387572544] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=6.030373573303223\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:21 INFO 140362387572544] Epoch[26] Batch [10]#011Speed: 520.73 samples/sec#011loss=6.030374\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:21 INFO 140362387572544] processed a total of 345 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422121.0004292, \"EndTime\": 1704422121.770651, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 769.5975303649902, \"count\": 1, \"min\": 769.5975303649902, \"max\": 769.5975303649902}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:21 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=448.22349799187083 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:21 INFO 140362387572544] #progress_metric: host=algo-1, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:21 INFO 140362387572544] #quality_metric: host=algo-1, epoch=26, train loss <loss>=6.589879296042702\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:21 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:21 INFO 140362387572544] Epoch[27] Batch[0] avg_epoch_loss=7.194855\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:21 INFO 140362387572544] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=7.194854736328125\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:22 INFO 140362387572544] Epoch[27] Batch[5] avg_epoch_loss=6.217908\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:22 INFO 140362387572544] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=6.217907667160034\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:22 INFO 140362387572544] Epoch[27] Batch [5]#011Speed: 570.03 samples/sec#011loss=6.217908\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:22 INFO 140362387572544] processed a total of 307 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422121.7707214, \"EndTime\": 1704422122.462867, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 691.7057037353516, \"count\": 1, \"min\": 691.7057037353516, \"max\": 691.7057037353516}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:22 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=443.7471665550106 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:22 INFO 140362387572544] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:22 INFO 140362387572544] #quality_metric: host=algo-1, epoch=27, train loss <loss>=5.633135223388672\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:22 INFO 140362387572544] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:22 INFO 140362387572544] Saved checkpoint to \"/opt/ml/model/state_7afbc6cc-a116-4753-a044-8f8e6a8e7424-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422122.4629571, \"EndTime\": 1704422122.4758725, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.139558792114258, \"count\": 1, \"min\": 12.139558792114258, \"max\": 12.139558792114258}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:22 INFO 140362387572544] Epoch[28] Batch[0] avg_epoch_loss=4.525711\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:22 INFO 140362387572544] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=4.5257110595703125\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:22 INFO 140362387572544] Epoch[28] Batch[5] avg_epoch_loss=5.726447\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:22 INFO 140362387572544] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=5.726447343826294\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:22 INFO 140362387572544] Epoch[28] Batch [5]#011Speed: 576.38 samples/sec#011loss=5.726447\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:23 INFO 140362387572544] Epoch[28] Batch[10] avg_epoch_loss=5.993917\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:23 INFO 140362387572544] #quality_metric: host=algo-1, epoch=28, batch=10 train loss <loss>=6.314880084991455\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:23 INFO 140362387572544] Epoch[28] Batch [10]#011Speed: 560.33 samples/sec#011loss=6.314880\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:23 INFO 140362387572544] processed a total of 349 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422122.4759445, \"EndTime\": 1704422123.2411973, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 765.1972770690918, \"count\": 1, \"min\": 765.1972770690918, \"max\": 765.1972770690918}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:23 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=456.0264579143208 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:23 INFO 140362387572544] #progress_metric: host=algo-1, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:23 INFO 140362387572544] #quality_metric: host=algo-1, epoch=28, train loss <loss>=5.99391677162864\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:23 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:23 INFO 140362387572544] Epoch[29] Batch[0] avg_epoch_loss=5.600883\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:23 INFO 140362387572544] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=5.600883483886719\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:23 INFO 140362387572544] Epoch[29] Batch[5] avg_epoch_loss=6.198092\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:23 INFO 140362387572544] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=6.198092063268025\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:23 INFO 140362387572544] Epoch[29] Batch [5]#011Speed: 600.00 samples/sec#011loss=6.198092\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:23 INFO 140362387572544] processed a total of 319 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422123.2412744, \"EndTime\": 1704422123.9249084, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 683.1991672515869, \"count\": 1, \"min\": 683.1991672515869, \"max\": 683.1991672515869}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:23 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=466.82906197915014 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:23 INFO 140362387572544] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:23 INFO 140362387572544] #quality_metric: host=algo-1, epoch=29, train loss <loss>=6.0699605464935305\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:23 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:24 INFO 140362387572544] Epoch[30] Batch[0] avg_epoch_loss=4.936926\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:24 INFO 140362387572544] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=4.936925888061523\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:24 INFO 140362387572544] Epoch[30] Batch[5] avg_epoch_loss=6.276929\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:24 INFO 140362387572544] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=6.276928742726644\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:24 INFO 140362387572544] Epoch[30] Batch [5]#011Speed: 571.16 samples/sec#011loss=6.276929\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:24 INFO 140362387572544] processed a total of 316 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422123.9250033, \"EndTime\": 1704422124.6149116, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 689.2879009246826, \"count\": 1, \"min\": 689.2879009246826, \"max\": 689.2879009246826}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:24 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=458.3572606601603 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:24 INFO 140362387572544] #progress_metric: host=algo-1, completed 31.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:24 INFO 140362387572544] #quality_metric: host=algo-1, epoch=30, train loss <loss>=6.3716428756713865\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:24 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:24 INFO 140362387572544] Epoch[31] Batch[0] avg_epoch_loss=4.413055\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:24 INFO 140362387572544] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=4.413054943084717\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:25 INFO 140362387572544] Epoch[31] Batch[5] avg_epoch_loss=4.986562\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:25 INFO 140362387572544] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=4.986561854680379\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:25 INFO 140362387572544] Epoch[31] Batch [5]#011Speed: 561.02 samples/sec#011loss=4.986562\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:25 INFO 140362387572544] processed a total of 306 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422124.6150026, \"EndTime\": 1704422125.3168976, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 701.1291980743408, \"count\": 1, \"min\": 701.1291980743408, \"max\": 701.1291980743408}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:25 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=436.3276888170585 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:25 INFO 140362387572544] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:25 INFO 140362387572544] #quality_metric: host=algo-1, epoch=31, train loss <loss>=5.326172924041748\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:25 INFO 140362387572544] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:25 INFO 140362387572544] Saved checkpoint to \"/opt/ml/model/state_bea52f8b-9a89-4a0f-b6e1-aac063af12bc-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422125.317034, \"EndTime\": 1704422125.3296857, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.167930603027344, \"count\": 1, \"min\": 12.167930603027344, \"max\": 12.167930603027344}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:25 INFO 140362387572544] Epoch[32] Batch[0] avg_epoch_loss=5.826892\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:25 INFO 140362387572544] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=5.826891899108887\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:25 INFO 140362387572544] Epoch[32] Batch[5] avg_epoch_loss=5.929658\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:25 INFO 140362387572544] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=5.929657856623332\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:25 INFO 140362387572544] Epoch[32] Batch [5]#011Speed: 605.59 samples/sec#011loss=5.929658\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:26 INFO 140362387572544] Epoch[32] Batch[10] avg_epoch_loss=5.879787\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:26 INFO 140362387572544] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=5.81994104385376\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:26 INFO 140362387572544] Epoch[32] Batch [10]#011Speed: 555.41 samples/sec#011loss=5.819941\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:26 INFO 140362387572544] processed a total of 321 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422125.329758, \"EndTime\": 1704422126.078465, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 748.6462593078613, \"count\": 1, \"min\": 748.6462593078613, \"max\": 748.6462593078613}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:26 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=428.6846464996383 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:26 INFO 140362387572544] #progress_metric: host=algo-1, completed 33.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:26 INFO 140362387572544] #quality_metric: host=algo-1, epoch=32, train loss <loss>=5.879786578091708\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:26 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:26 INFO 140362387572544] Epoch[33] Batch[0] avg_epoch_loss=6.361410\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:26 INFO 140362387572544] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=6.361410140991211\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:26 INFO 140362387572544] Epoch[33] Batch[5] avg_epoch_loss=7.042631\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:26 INFO 140362387572544] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=7.042631228764852\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:26 INFO 140362387572544] Epoch[33] Batch [5]#011Speed: 573.22 samples/sec#011loss=7.042631\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:26 INFO 140362387572544] processed a total of 307 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422126.0785837, \"EndTime\": 1704422126.7682858, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 689.2721652984619, \"count\": 1, \"min\": 689.2721652984619, \"max\": 689.2721652984619}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:26 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=445.3164878808608 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:26 INFO 140362387572544] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:26 INFO 140362387572544] #quality_metric: host=algo-1, epoch=33, train loss <loss>=7.4536614418029785\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:26 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:26 INFO 140362387572544] Epoch[34] Batch[0] avg_epoch_loss=7.907494\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:26 INFO 140362387572544] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=7.907493591308594\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:27 INFO 140362387572544] Epoch[34] Batch[5] avg_epoch_loss=6.985214\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:27 INFO 140362387572544] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=6.985213836034139\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:27 INFO 140362387572544] Epoch[34] Batch [5]#011Speed: 570.42 samples/sec#011loss=6.985214\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:27 INFO 140362387572544] Epoch[34] Batch[10] avg_epoch_loss=6.430810\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:27 INFO 140362387572544] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=5.7655257225036625\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:27 INFO 140362387572544] Epoch[34] Batch [10]#011Speed: 575.95 samples/sec#011loss=5.765526\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:27 INFO 140362387572544] processed a total of 330 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422126.7683709, \"EndTime\": 1704422127.5220282, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 753.1576156616211, \"count\": 1, \"min\": 753.1576156616211, \"max\": 753.1576156616211}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:27 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=438.0847107483792 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:27 INFO 140362387572544] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:27 INFO 140362387572544] #quality_metric: host=algo-1, epoch=34, train loss <loss>=6.43081014806574\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:27 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:27 INFO 140362387572544] Epoch[35] Batch[0] avg_epoch_loss=5.700838\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:27 INFO 140362387572544] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=5.700838088989258\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:27 INFO 140362387572544] Epoch[35] Batch[5] avg_epoch_loss=6.285032\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:27 INFO 140362387572544] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=6.285032272338867\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:27 INFO 140362387572544] Epoch[35] Batch [5]#011Speed: 583.09 samples/sec#011loss=6.285032\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:28 INFO 140362387572544] Epoch[35] Batch[10] avg_epoch_loss=6.237543\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:28 INFO 140362387572544] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=6.180554962158203\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:28 INFO 140362387572544] Epoch[35] Batch [10]#011Speed: 580.80 samples/sec#011loss=6.180555\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:28 INFO 140362387572544] processed a total of 327 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422127.5221124, \"EndTime\": 1704422128.2657454, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 743.0799007415771, \"count\": 1, \"min\": 743.0799007415771, \"max\": 743.0799007415771}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:28 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=439.9757893203803 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:28 INFO 140362387572544] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:28 INFO 140362387572544] #quality_metric: host=algo-1, epoch=35, train loss <loss>=6.23754258589311\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:28 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:28 INFO 140362387572544] Epoch[36] Batch[0] avg_epoch_loss=5.835394\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:28 INFO 140362387572544] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=5.835394382476807\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:28 INFO 140362387572544] Epoch[36] Batch[5] avg_epoch_loss=6.265278\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:28 INFO 140362387572544] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=6.265277703603108\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:28 INFO 140362387572544] Epoch[36] Batch [5]#011Speed: 583.20 samples/sec#011loss=6.265278\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:28 INFO 140362387572544] processed a total of 315 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422128.26585, \"EndTime\": 1704422128.957743, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 691.4515495300293, \"count\": 1, \"min\": 691.4515495300293, \"max\": 691.4515495300293}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:28 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=455.46097814058834 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:28 INFO 140362387572544] #progress_metric: host=algo-1, completed 37.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:28 INFO 140362387572544] #quality_metric: host=algo-1, epoch=36, train loss <loss>=6.490609645843506\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:28 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:29 INFO 140362387572544] Epoch[37] Batch[0] avg_epoch_loss=5.956778\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:29 INFO 140362387572544] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=5.956777572631836\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:29 INFO 140362387572544] Epoch[37] Batch[5] avg_epoch_loss=6.739287\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:29 INFO 140362387572544] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=6.73928689956665\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:29 INFO 140362387572544] Epoch[37] Batch [5]#011Speed: 593.41 samples/sec#011loss=6.739287\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:29 INFO 140362387572544] processed a total of 306 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422128.9578557, \"EndTime\": 1704422129.6370938, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 678.3719062805176, \"count\": 1, \"min\": 678.3719062805176, \"max\": 678.3719062805176}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:29 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=450.9920308603431 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:29 INFO 140362387572544] #progress_metric: host=algo-1, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:29 INFO 140362387572544] #quality_metric: host=algo-1, epoch=37, train loss <loss>=7.011852121353149\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:29 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:29 INFO 140362387572544] Epoch[38] Batch[0] avg_epoch_loss=7.935620\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:29 INFO 140362387572544] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=7.935620307922363\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:30 INFO 140362387572544] Epoch[38] Batch[5] avg_epoch_loss=6.565821\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:30 INFO 140362387572544] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=6.5658213297526045\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:30 INFO 140362387572544] Epoch[38] Batch [5]#011Speed: 578.68 samples/sec#011loss=6.565821\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:30 INFO 140362387572544] processed a total of 294 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422129.6371856, \"EndTime\": 1704422130.3222897, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 684.6232414245605, \"count\": 1, \"min\": 684.6232414245605, \"max\": 684.6232414245605}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:30 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=429.34774885188136 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:30 INFO 140362387572544] #progress_metric: host=algo-1, completed 39.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:30 INFO 140362387572544] #quality_metric: host=algo-1, epoch=38, train loss <loss>=6.049503421783447\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:30 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:30 INFO 140362387572544] Epoch[39] Batch[0] avg_epoch_loss=8.506847\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:30 INFO 140362387572544] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=8.506847381591797\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:30 INFO 140362387572544] Epoch[39] Batch[5] avg_epoch_loss=6.797652\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:30 INFO 140362387572544] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=6.797652165095012\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:30 INFO 140362387572544] Epoch[39] Batch [5]#011Speed: 590.08 samples/sec#011loss=6.797652\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:31 INFO 140362387572544] processed a total of 294 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422130.3223848, \"EndTime\": 1704422131.0255134, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 702.6231288909912, \"count\": 1, \"min\": 702.6231288909912, \"max\": 702.6231288909912}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:31 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=418.34881179644054 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:31 INFO 140362387572544] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:31 INFO 140362387572544] #quality_metric: host=algo-1, epoch=39, train loss <loss>=6.2759013175964355\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:31 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:31 INFO 140362387572544] Epoch[40] Batch[0] avg_epoch_loss=5.910100\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:31 INFO 140362387572544] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=5.910099506378174\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:31 INFO 140362387572544] Epoch[40] Batch[5] avg_epoch_loss=5.838051\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:31 INFO 140362387572544] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=5.838050921758016\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:31 INFO 140362387572544] Epoch[40] Batch [5]#011Speed: 553.68 samples/sec#011loss=5.838051\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:31 INFO 140362387572544] processed a total of 313 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422131.0256057, \"EndTime\": 1704422131.7298481, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 703.6740779876709, \"count\": 1, \"min\": 703.6740779876709, \"max\": 703.6740779876709}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:31 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=444.7263796398739 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:31 INFO 140362387572544] #progress_metric: host=algo-1, completed 41.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:31 INFO 140362387572544] #quality_metric: host=algo-1, epoch=40, train loss <loss>=5.880579900741577\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:31 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:31 INFO 140362387572544] Epoch[41] Batch[0] avg_epoch_loss=5.380222\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:31 INFO 140362387572544] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=5.380222320556641\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:32 INFO 140362387572544] Epoch[41] Batch[5] avg_epoch_loss=5.754738\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:32 INFO 140362387572544] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=5.754737774531047\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:32 INFO 140362387572544] Epoch[41] Batch [5]#011Speed: 570.54 samples/sec#011loss=5.754738\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:32 INFO 140362387572544] Epoch[41] Batch[10] avg_epoch_loss=5.853395\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:32 INFO 140362387572544] #quality_metric: host=algo-1, epoch=41, batch=10 train loss <loss>=5.971782970428467\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:32 INFO 140362387572544] Epoch[41] Batch [10]#011Speed: 586.28 samples/sec#011loss=5.971783\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:32 INFO 140362387572544] processed a total of 324 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422131.729936, \"EndTime\": 1704422132.4784214, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 747.7431297302246, \"count\": 1, \"min\": 747.7431297302246, \"max\": 747.7431297302246}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:32 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=433.2237632625424 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:32 INFO 140362387572544] #progress_metric: host=algo-1, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:32 INFO 140362387572544] #quality_metric: host=algo-1, epoch=41, train loss <loss>=5.8533946817571465\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:32 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:32 INFO 140362387572544] Epoch[42] Batch[0] avg_epoch_loss=4.862122\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:32 INFO 140362387572544] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=4.86212158203125\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:32 INFO 140362387572544] Epoch[42] Batch[5] avg_epoch_loss=5.828980\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:32 INFO 140362387572544] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=5.828980366388957\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:32 INFO 140362387572544] Epoch[42] Batch [5]#011Speed: 530.08 samples/sec#011loss=5.828980\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:33 INFO 140362387572544] processed a total of 315 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422132.478517, \"EndTime\": 1704422133.2036967, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 724.6475219726562, \"count\": 1, \"min\": 724.6475219726562, \"max\": 724.6475219726562}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:33 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=434.6153069317418 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:33 INFO 140362387572544] #progress_metric: host=algo-1, completed 43.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:33 INFO 140362387572544] #quality_metric: host=algo-1, epoch=42, train loss <loss>=5.872129487991333\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:33 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:33 INFO 140362387572544] Epoch[43] Batch[0] avg_epoch_loss=7.302391\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:33 INFO 140362387572544] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=7.302391052246094\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:33 INFO 140362387572544] Epoch[43] Batch[5] avg_epoch_loss=5.948363\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:33 INFO 140362387572544] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=5.948362747828166\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:33 INFO 140362387572544] Epoch[43] Batch [5]#011Speed: 571.99 samples/sec#011loss=5.948363\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:33 INFO 140362387572544] Epoch[43] Batch[10] avg_epoch_loss=6.118720\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:33 INFO 140362387572544] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=6.323149585723877\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:33 INFO 140362387572544] Epoch[43] Batch [10]#011Speed: 560.95 samples/sec#011loss=6.323150\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:34 INFO 140362387572544] processed a total of 356 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422133.2037878, \"EndTime\": 1704422134.008267, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 803.7974834442139, \"count\": 1, \"min\": 803.7974834442139, \"max\": 803.7974834442139}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:34 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=442.80266920198545 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:34 INFO 140362387572544] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:34 INFO 140362387572544] #quality_metric: host=algo-1, epoch=43, train loss <loss>=5.889945844809215\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:34 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:34 INFO 140362387572544] Epoch[44] Batch[0] avg_epoch_loss=7.659744\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:34 INFO 140362387572544] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=7.659743785858154\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:34 INFO 140362387572544] Epoch[44] Batch[5] avg_epoch_loss=5.788459\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:34 INFO 140362387572544] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=5.788459459940593\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:34 INFO 140362387572544] Epoch[44] Batch [5]#011Speed: 577.99 samples/sec#011loss=5.788459\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:34 INFO 140362387572544] Epoch[44] Batch[10] avg_epoch_loss=5.822627\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:34 INFO 140362387572544] #quality_metric: host=algo-1, epoch=44, batch=10 train loss <loss>=5.863628768920899\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:34 INFO 140362387572544] Epoch[44] Batch [10]#011Speed: 567.01 samples/sec#011loss=5.863629\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:34 INFO 140362387572544] processed a total of 327 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422134.0083969, \"EndTime\": 1704422134.7578156, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 748.8446235656738, \"count\": 1, \"min\": 748.8446235656738, \"max\": 748.8446235656738}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:34 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=436.5715056929409 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:34 INFO 140362387572544] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:34 INFO 140362387572544] #quality_metric: host=algo-1, epoch=44, train loss <loss>=5.822627327658913\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:34 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:34 INFO 140362387572544] Epoch[45] Batch[0] avg_epoch_loss=5.050505\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:34 INFO 140362387572544] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=5.050504684448242\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:35 INFO 140362387572544] Epoch[45] Batch[5] avg_epoch_loss=5.430225\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:35 INFO 140362387572544] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=5.430224895477295\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:35 INFO 140362387572544] Epoch[45] Batch [5]#011Speed: 587.68 samples/sec#011loss=5.430225\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:35 INFO 140362387572544] processed a total of 307 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422134.7579045, \"EndTime\": 1704422135.4430945, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 684.7503185272217, \"count\": 1, \"min\": 684.7503185272217, \"max\": 684.7503185272217}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:35 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=448.25338465506996 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:35 INFO 140362387572544] #progress_metric: host=algo-1, completed 46.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:35 INFO 140362387572544] #quality_metric: host=algo-1, epoch=45, train loss <loss>=6.043376159667969\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:35 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:35 INFO 140362387572544] Epoch[46] Batch[0] avg_epoch_loss=7.175788\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:35 INFO 140362387572544] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=7.175788402557373\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:35 INFO 140362387572544] Epoch[46] Batch[5] avg_epoch_loss=5.945753\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:35 INFO 140362387572544] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=5.94575309753418\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:35 INFO 140362387572544] Epoch[46] Batch [5]#011Speed: 600.09 samples/sec#011loss=5.945753\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:36 INFO 140362387572544] Epoch[46] Batch[10] avg_epoch_loss=6.074243\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:36 INFO 140362387572544] #quality_metric: host=algo-1, epoch=46, batch=10 train loss <loss>=6.228430271148682\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:36 INFO 140362387572544] Epoch[46] Batch [10]#011Speed: 474.84 samples/sec#011loss=6.228430\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:36 INFO 140362387572544] processed a total of 327 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422135.4431849, \"EndTime\": 1704422136.2385068, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 794.8379516601562, \"count\": 1, \"min\": 794.8379516601562, \"max\": 794.8379516601562}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:36 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=411.33502481438694 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:36 INFO 140362387572544] #progress_metric: host=algo-1, completed 47.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:36 INFO 140362387572544] #quality_metric: host=algo-1, epoch=46, train loss <loss>=6.074242721904408\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:36 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:36 INFO 140362387572544] Epoch[47] Batch[0] avg_epoch_loss=5.479903\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:36 INFO 140362387572544] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=5.479902744293213\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:36 INFO 140362387572544] Epoch[47] Batch[5] avg_epoch_loss=6.230197\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:36 INFO 140362387572544] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=6.230196714401245\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:36 INFO 140362387572544] Epoch[47] Batch [5]#011Speed: 609.98 samples/sec#011loss=6.230197\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:36 INFO 140362387572544] processed a total of 296 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422136.2385995, \"EndTime\": 1704422136.9260383, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 686.9115829467773, \"count\": 1, \"min\": 686.9115829467773, \"max\": 686.9115829467773}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:36 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=430.83067452089495 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:36 INFO 140362387572544] #progress_metric: host=algo-1, completed 48.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:36 INFO 140362387572544] #quality_metric: host=algo-1, epoch=47, train loss <loss>=6.052114009857178\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:36 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:37 INFO 140362387572544] Epoch[48] Batch[0] avg_epoch_loss=6.595092\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:37 INFO 140362387572544] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=6.595092296600342\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:37 INFO 140362387572544] Epoch[48] Batch[5] avg_epoch_loss=6.138815\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:37 INFO 140362387572544] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=6.1388148466746015\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:37 INFO 140362387572544] Epoch[48] Batch [5]#011Speed: 578.49 samples/sec#011loss=6.138815\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:37 INFO 140362387572544] Epoch[48] Batch[10] avg_epoch_loss=5.870573\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:37 INFO 140362387572544] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=5.548682022094726\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:37 INFO 140362387572544] Epoch[48] Batch [10]#011Speed: 549.78 samples/sec#011loss=5.548682\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:37 INFO 140362387572544] processed a total of 345 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422136.9261305, \"EndTime\": 1704422137.6886375, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 761.6605758666992, \"count\": 1, \"min\": 761.6605758666992, \"max\": 761.6605758666992}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:37 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=452.88083480795126 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:37 INFO 140362387572544] #progress_metric: host=algo-1, completed 49.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:37 INFO 140362387572544] #quality_metric: host=algo-1, epoch=48, train loss <loss>=5.870572653683749\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:37 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:37 INFO 140362387572544] Epoch[49] Batch[0] avg_epoch_loss=5.875496\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:37 INFO 140362387572544] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=5.875495910644531\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:38 INFO 140362387572544] Epoch[49] Batch[5] avg_epoch_loss=6.026617\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:38 INFO 140362387572544] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=6.02661673227946\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:38 INFO 140362387572544] Epoch[49] Batch [5]#011Speed: 581.98 samples/sec#011loss=6.026617\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:38 INFO 140362387572544] processed a total of 320 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422137.6887274, \"EndTime\": 1704422138.3781984, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 688.9951229095459, \"count\": 1, \"min\": 688.9951229095459, \"max\": 688.9951229095459}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:38 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=464.3498645882778 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:38 INFO 140362387572544] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:38 INFO 140362387572544] #quality_metric: host=algo-1, epoch=49, train loss <loss>=5.919544219970703\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:38 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:38 INFO 140362387572544] Epoch[50] Batch[0] avg_epoch_loss=5.623898\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:38 INFO 140362387572544] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=5.623898029327393\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:38 INFO 140362387572544] Epoch[50] Batch[5] avg_epoch_loss=5.694950\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:38 INFO 140362387572544] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=5.694949785868327\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:38 INFO 140362387572544] Epoch[50] Batch [5]#011Speed: 593.41 samples/sec#011loss=5.694950\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:39 INFO 140362387572544] Epoch[50] Batch[10] avg_epoch_loss=5.769043\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:39 INFO 140362387572544] #quality_metric: host=algo-1, epoch=50, batch=10 train loss <loss>=5.857954216003418\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:39 INFO 140362387572544] Epoch[50] Batch [10]#011Speed: 547.96 samples/sec#011loss=5.857954\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:39 INFO 140362387572544] processed a total of 338 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422138.3782933, \"EndTime\": 1704422139.132112, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 753.1752586364746, \"count\": 1, \"min\": 753.1752586364746, \"max\": 753.1752586364746}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:39 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=448.68875662861853 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:39 INFO 140362387572544] #progress_metric: host=algo-1, completed 51.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:39 INFO 140362387572544] #quality_metric: host=algo-1, epoch=50, train loss <loss>=5.769042708657005\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:39 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:39 INFO 140362387572544] Epoch[51] Batch[0] avg_epoch_loss=5.392027\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:39 INFO 140362387572544] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=5.392027378082275\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:39 INFO 140362387572544] Epoch[51] Batch[5] avg_epoch_loss=6.138678\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:39 INFO 140362387572544] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=6.138678153355916\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:39 INFO 140362387572544] Epoch[51] Batch [5]#011Speed: 585.57 samples/sec#011loss=6.138678\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:39 INFO 140362387572544] Epoch[51] Batch[10] avg_epoch_loss=6.940337\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:39 INFO 140362387572544] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=7.902327156066894\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:39 INFO 140362387572544] Epoch[51] Batch [10]#011Speed: 569.75 samples/sec#011loss=7.902327\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:39 INFO 140362387572544] processed a total of 330 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422139.1322, \"EndTime\": 1704422139.8824947, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 749.7079372406006, \"count\": 1, \"min\": 749.7079372406006, \"max\": 749.7079372406006}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:39 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=440.07344517985007 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:39 INFO 140362387572544] #progress_metric: host=algo-1, completed 52.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:39 INFO 140362387572544] #quality_metric: host=algo-1, epoch=51, train loss <loss>=6.940336790951815\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:39 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:40 INFO 140362387572544] Epoch[52] Batch[0] avg_epoch_loss=5.241585\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:40 INFO 140362387572544] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=5.2415852546691895\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:40 INFO 140362387572544] Epoch[52] Batch[5] avg_epoch_loss=5.847510\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:40 INFO 140362387572544] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=5.84751033782959\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:40 INFO 140362387572544] Epoch[52] Batch [5]#011Speed: 560.85 samples/sec#011loss=5.847510\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:40 INFO 140362387572544] Epoch[52] Batch[10] avg_epoch_loss=5.416162\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:40 INFO 140362387572544] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=4.898543024063111\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:40 INFO 140362387572544] Epoch[52] Batch [10]#011Speed: 575.11 samples/sec#011loss=4.898543\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:40 INFO 140362387572544] processed a total of 321 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422139.8826213, \"EndTime\": 1704422140.6423523, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 759.0234279632568, \"count\": 1, \"min\": 759.0234279632568, \"max\": 759.0234279632568}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:40 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=422.8325555277793 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:40 INFO 140362387572544] #progress_metric: host=algo-1, completed 53.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:40 INFO 140362387572544] #quality_metric: host=algo-1, epoch=52, train loss <loss>=5.416161558844826\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:40 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:40 INFO 140362387572544] Epoch[53] Batch[0] avg_epoch_loss=5.517561\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:40 INFO 140362387572544] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=5.517561435699463\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:41 INFO 140362387572544] Epoch[53] Batch[5] avg_epoch_loss=6.124690\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:41 INFO 140362387572544] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=6.124690294265747\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:41 INFO 140362387572544] Epoch[53] Batch [5]#011Speed: 569.26 samples/sec#011loss=6.124690\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:41 INFO 140362387572544] processed a total of 301 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422140.642449, \"EndTime\": 1704422141.3498683, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 706.8839073181152, \"count\": 1, \"min\": 706.8839073181152, \"max\": 706.8839073181152}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:41 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=425.7272023416108 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:41 INFO 140362387572544] #progress_metric: host=algo-1, completed 54.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:41 INFO 140362387572544] #quality_metric: host=algo-1, epoch=53, train loss <loss>=6.835354137420654\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:41 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:41 INFO 140362387572544] Epoch[54] Batch[0] avg_epoch_loss=5.610944\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:41 INFO 140362387572544] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=5.6109442710876465\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:41 INFO 140362387572544] Epoch[54] Batch[5] avg_epoch_loss=5.517957\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:41 INFO 140362387572544] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=5.517956733703613\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:41 INFO 140362387572544] Epoch[54] Batch [5]#011Speed: 600.72 samples/sec#011loss=5.517957\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:42 INFO 140362387572544] processed a total of 305 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422141.349963, \"EndTime\": 1704422142.0259006, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 675.2347946166992, \"count\": 1, \"min\": 675.2347946166992, \"max\": 675.2347946166992}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:42 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=451.6036828324527 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:42 INFO 140362387572544] #progress_metric: host=algo-1, completed 55.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:42 INFO 140362387572544] #quality_metric: host=algo-1, epoch=54, train loss <loss>=5.883700370788574\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:42 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:42 INFO 140362387572544] Epoch[55] Batch[0] avg_epoch_loss=4.707951\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:42 INFO 140362387572544] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=4.707950592041016\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:42 INFO 140362387572544] Epoch[55] Batch[5] avg_epoch_loss=5.282409\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:42 INFO 140362387572544] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=5.282408634821574\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:42 INFO 140362387572544] Epoch[55] Batch [5]#011Speed: 600.97 samples/sec#011loss=5.282409\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:42 INFO 140362387572544] processed a total of 310 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422142.0259938, \"EndTime\": 1704422142.7043784, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 677.7238845825195, \"count\": 1, \"min\": 677.7238845825195, \"max\": 677.7238845825195}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:42 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=457.325577550907 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:42 INFO 140362387572544] #progress_metric: host=algo-1, completed 56.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:42 INFO 140362387572544] #quality_metric: host=algo-1, epoch=55, train loss <loss>=5.806913185119629\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:42 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:42 INFO 140362387572544] Epoch[56] Batch[0] avg_epoch_loss=5.470768\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:42 INFO 140362387572544] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=5.470768451690674\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:43 INFO 140362387572544] Epoch[56] Batch[5] avg_epoch_loss=5.841331\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:43 INFO 140362387572544] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=5.8413310050964355\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:43 INFO 140362387572544] Epoch[56] Batch [5]#011Speed: 528.62 samples/sec#011loss=5.841331\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:43 INFO 140362387572544] Epoch[56] Batch[10] avg_epoch_loss=5.854928\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:43 INFO 140362387572544] #quality_metric: host=algo-1, epoch=56, batch=10 train loss <loss>=5.871244335174561\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:43 INFO 140362387572544] Epoch[56] Batch [10]#011Speed: 567.75 samples/sec#011loss=5.871244\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:43 INFO 140362387572544] processed a total of 322 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422142.704469, \"EndTime\": 1704422143.482447, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 777.4748802185059, \"count\": 1, \"min\": 777.4748802185059, \"max\": 777.4748802185059}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:43 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=414.09919437796935 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:43 INFO 140362387572544] #progress_metric: host=algo-1, completed 57.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:43 INFO 140362387572544] #quality_metric: host=algo-1, epoch=56, train loss <loss>=5.854927973313765\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:43 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:43 INFO 140362387572544] Epoch[57] Batch[0] avg_epoch_loss=5.837925\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:43 INFO 140362387572544] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=5.837924957275391\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:43 INFO 140362387572544] Epoch[57] Batch[5] avg_epoch_loss=6.521558\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:43 INFO 140362387572544] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=6.521557966868083\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:43 INFO 140362387572544] Epoch[57] Batch [5]#011Speed: 585.38 samples/sec#011loss=6.521558\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:44 INFO 140362387572544] processed a total of 311 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422143.4825263, \"EndTime\": 1704422144.1730096, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 690.0529861450195, \"count\": 1, \"min\": 690.0529861450195, \"max\": 690.0529861450195}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:44 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=450.605801440014 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:44 INFO 140362387572544] #progress_metric: host=algo-1, completed 58.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:44 INFO 140362387572544] #quality_metric: host=algo-1, epoch=57, train loss <loss>=6.104307794570923\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:44 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:44 INFO 140362387572544] Epoch[58] Batch[0] avg_epoch_loss=6.147709\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:44 INFO 140362387572544] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=6.147708892822266\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:44 INFO 140362387572544] Epoch[58] Batch[5] avg_epoch_loss=6.077591\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:44 INFO 140362387572544] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=6.07759149869283\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:44 INFO 140362387572544] Epoch[58] Batch [5]#011Speed: 574.72 samples/sec#011loss=6.077591\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:44 INFO 140362387572544] processed a total of 319 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422144.1730952, \"EndTime\": 1704422144.862549, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 688.6703968048096, \"count\": 1, \"min\": 688.6703968048096, \"max\": 688.6703968048096}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:44 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=463.12613286489386 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:44 INFO 140362387572544] #progress_metric: host=algo-1, completed 59.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:44 INFO 140362387572544] #quality_metric: host=algo-1, epoch=58, train loss <loss>=5.956885194778442\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:44 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:45 INFO 140362387572544] Epoch[59] Batch[0] avg_epoch_loss=5.454934\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:45 INFO 140362387572544] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=5.454934120178223\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:45 INFO 140362387572544] Epoch[59] Batch[5] avg_epoch_loss=6.027330\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:45 INFO 140362387572544] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=6.027329683303833\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:45 INFO 140362387572544] Epoch[59] Batch [5]#011Speed: 578.32 samples/sec#011loss=6.027330\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:45 INFO 140362387572544] Epoch[59] Batch[10] avg_epoch_loss=5.710937\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:45 INFO 140362387572544] #quality_metric: host=algo-1, epoch=59, batch=10 train loss <loss>=5.3312663555145265\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:45 INFO 140362387572544] Epoch[59] Batch [10]#011Speed: 563.46 samples/sec#011loss=5.331266\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:45 INFO 140362387572544] processed a total of 337 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422144.8626342, \"EndTime\": 1704422145.612872, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 749.6616840362549, \"count\": 1, \"min\": 749.6616840362549, \"max\": 749.6616840362549}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:45 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=449.457923669637 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:45 INFO 140362387572544] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:45 INFO 140362387572544] #quality_metric: host=algo-1, epoch=59, train loss <loss>=5.710937261581421\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:45 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:45 INFO 140362387572544] Epoch[60] Batch[0] avg_epoch_loss=6.371002\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:45 INFO 140362387572544] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=6.371002197265625\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:46 INFO 140362387572544] Epoch[60] Batch[5] avg_epoch_loss=6.613263\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:46 INFO 140362387572544] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=6.613262573877971\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:46 INFO 140362387572544] Epoch[60] Batch [5]#011Speed: 578.17 samples/sec#011loss=6.613263\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:46 INFO 140362387572544] processed a total of 302 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422145.6129646, \"EndTime\": 1704422146.3053417, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 691.962480545044, \"count\": 1, \"min\": 691.962480545044, \"max\": 691.962480545044}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:46 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=436.3533984917027 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:46 INFO 140362387572544] #progress_metric: host=algo-1, completed 61.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:46 INFO 140362387572544] #quality_metric: host=algo-1, epoch=60, train loss <loss>=6.437744092941284\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:46 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:46 INFO 140362387572544] Epoch[61] Batch[0] avg_epoch_loss=4.762377\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:46 INFO 140362387572544] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=4.76237678527832\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:46 INFO 140362387572544] Epoch[61] Batch[5] avg_epoch_loss=5.486107\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:46 INFO 140362387572544] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=5.486107349395752\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:46 INFO 140362387572544] Epoch[61] Batch [5]#011Speed: 591.37 samples/sec#011loss=5.486107\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:46 INFO 140362387572544] processed a total of 299 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422146.3054373, \"EndTime\": 1704422146.9870274, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 680.8891296386719, \"count\": 1, \"min\": 680.8891296386719, \"max\": 680.8891296386719}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:46 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=439.0460679180497 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:46 INFO 140362387572544] #progress_metric: host=algo-1, completed 62.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:46 INFO 140362387572544] #quality_metric: host=algo-1, epoch=61, train loss <loss>=6.457043218612671\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:46 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:47 INFO 140362387572544] Epoch[62] Batch[0] avg_epoch_loss=5.115191\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:47 INFO 140362387572544] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=5.115191459655762\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:47 INFO 140362387572544] Epoch[62] Batch[5] avg_epoch_loss=5.695012\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:47 INFO 140362387572544] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=5.695011615753174\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:47 INFO 140362387572544] Epoch[62] Batch [5]#011Speed: 596.32 samples/sec#011loss=5.695012\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:47 INFO 140362387572544] Epoch[62] Batch[10] avg_epoch_loss=6.178906\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:47 INFO 140362387572544] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=6.759579181671143\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:47 INFO 140362387572544] Epoch[62] Batch [10]#011Speed: 568.04 samples/sec#011loss=6.759579\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:47 INFO 140362387572544] processed a total of 341 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422146.9871185, \"EndTime\": 1704422147.7366471, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 748.8594055175781, \"count\": 1, \"min\": 748.8594055175781, \"max\": 748.8594055175781}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:47 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=455.2836539639945 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:47 INFO 140362387572544] #progress_metric: host=algo-1, completed 63.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:47 INFO 140362387572544] #quality_metric: host=algo-1, epoch=62, train loss <loss>=6.178905963897705\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:47 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:47 INFO 140362387572544] Epoch[63] Batch[0] avg_epoch_loss=5.199441\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:47 INFO 140362387572544] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=5.199440956115723\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:48 INFO 140362387572544] Epoch[63] Batch[5] avg_epoch_loss=6.417133\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:48 INFO 140362387572544] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=6.417132536570231\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:48 INFO 140362387572544] Epoch[63] Batch [5]#011Speed: 591.15 samples/sec#011loss=6.417133\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:48 INFO 140362387572544] processed a total of 282 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422147.7367322, \"EndTime\": 1704422148.363246, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 625.9734630584717, \"count\": 1, \"min\": 625.9734630584717, \"max\": 625.9734630584717}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:48 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=450.36127634006795 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:48 INFO 140362387572544] #progress_metric: host=algo-1, completed 64.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:48 INFO 140362387572544] #quality_metric: host=algo-1, epoch=63, train loss <loss>=6.1592780749003095\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:48 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:48 INFO 140362387572544] Epoch[64] Batch[0] avg_epoch_loss=6.387609\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:48 INFO 140362387572544] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=6.387609481811523\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:48 INFO 140362387572544] Epoch[64] Batch[5] avg_epoch_loss=6.039633\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:48 INFO 140362387572544] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=6.039632956186931\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:48 INFO 140362387572544] Epoch[64] Batch [5]#011Speed: 580.85 samples/sec#011loss=6.039633\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:49 INFO 140362387572544] Epoch[64] Batch[10] avg_epoch_loss=6.422849\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:49 INFO 140362387572544] #quality_metric: host=algo-1, epoch=64, batch=10 train loss <loss>=6.882709264755249\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:49 INFO 140362387572544] Epoch[64] Batch [10]#011Speed: 580.75 samples/sec#011loss=6.882709\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:49 INFO 140362387572544] processed a total of 335 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422148.3633902, \"EndTime\": 1704422149.1113546, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 747.2717761993408, \"count\": 1, \"min\": 747.2717761993408, \"max\": 747.2717761993408}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:49 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=448.2150372855447 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:49 INFO 140362387572544] #progress_metric: host=algo-1, completed 65.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:49 INFO 140362387572544] #quality_metric: host=algo-1, epoch=64, train loss <loss>=6.422849460081621\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:49 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:49 INFO 140362387572544] Epoch[65] Batch[0] avg_epoch_loss=6.771221\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:49 INFO 140362387572544] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=6.771221160888672\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:49 INFO 140362387572544] Epoch[65] Batch[5] avg_epoch_loss=6.276906\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:49 INFO 140362387572544] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=6.276906331380208\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:49 INFO 140362387572544] Epoch[65] Batch [5]#011Speed: 606.35 samples/sec#011loss=6.276906\u001b[0m\n",
      "\n",
      "2024-01-05 02:35:59 Uploading - Uploading generated training model\u001b[34m[01/05/2024 02:35:49 INFO 140362387572544] processed a total of 311 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422149.1114504, \"EndTime\": 1704422149.7911918, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 679.0518760681152, \"count\": 1, \"min\": 679.0518760681152, \"max\": 679.0518760681152}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:49 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=457.8633944401212 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:49 INFO 140362387572544] #progress_metric: host=algo-1, completed 66.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:49 INFO 140362387572544] #quality_metric: host=algo-1, epoch=65, train loss <loss>=6.31110782623291\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:49 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:49 INFO 140362387572544] Epoch[66] Batch[0] avg_epoch_loss=5.194344\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:49 INFO 140362387572544] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=5.194343566894531\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:50 INFO 140362387572544] Epoch[66] Batch[5] avg_epoch_loss=4.955207\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:50 INFO 140362387572544] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=4.955207109451294\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:50 INFO 140362387572544] Epoch[66] Batch [5]#011Speed: 600.16 samples/sec#011loss=4.955207\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:50 INFO 140362387572544] Epoch[66] Batch[10] avg_epoch_loss=5.366138\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:50 INFO 140362387572544] #quality_metric: host=algo-1, epoch=66, batch=10 train loss <loss>=5.859255981445313\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:50 INFO 140362387572544] Epoch[66] Batch [10]#011Speed: 575.63 samples/sec#011loss=5.859256\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:50 INFO 140362387572544] processed a total of 332 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422149.7913327, \"EndTime\": 1704422150.5328124, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 740.807056427002, \"count\": 1, \"min\": 740.807056427002, \"max\": 740.807056427002}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:50 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=448.0675871433582 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:50 INFO 140362387572544] #progress_metric: host=algo-1, completed 67.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:50 INFO 140362387572544] #quality_metric: host=algo-1, epoch=66, train loss <loss>=5.36613841490312\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:50 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:50 INFO 140362387572544] Epoch[67] Batch[0] avg_epoch_loss=6.262895\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:50 INFO 140362387572544] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=6.262894630432129\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:51 INFO 140362387572544] Epoch[67] Batch[5] avg_epoch_loss=6.131637\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:51 INFO 140362387572544] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=6.131636619567871\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:51 INFO 140362387572544] Epoch[67] Batch [5]#011Speed: 573.67 samples/sec#011loss=6.131637\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:51 INFO 140362387572544] processed a total of 313 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422150.5329015, \"EndTime\": 1704422151.244035, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 710.585355758667, \"count\": 1, \"min\": 710.585355758667, \"max\": 710.585355758667}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:51 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=440.3947372040466 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:51 INFO 140362387572544] #progress_metric: host=algo-1, completed 68.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:51 INFO 140362387572544] #quality_metric: host=algo-1, epoch=67, train loss <loss>=6.310545635223389\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:51 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:51 INFO 140362387572544] Epoch[68] Batch[0] avg_epoch_loss=4.094704\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:51 INFO 140362387572544] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=4.0947041511535645\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:51 INFO 140362387572544] Epoch[68] Batch[5] avg_epoch_loss=5.993443\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:51 INFO 140362387572544] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=5.99344261487325\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:51 INFO 140362387572544] Epoch[68] Batch [5]#011Speed: 549.26 samples/sec#011loss=5.993443\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:51 INFO 140362387572544] processed a total of 317 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422151.2441337, \"EndTime\": 1704422151.967795, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 723.1712341308594, \"count\": 1, \"min\": 723.1712341308594, \"max\": 723.1712341308594}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:51 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=438.2719459936975 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:51 INFO 140362387572544] #progress_metric: host=algo-1, completed 69.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:51 INFO 140362387572544] #quality_metric: host=algo-1, epoch=68, train loss <loss>=5.928041410446167\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:51 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:52 INFO 140362387572544] Epoch[69] Batch[0] avg_epoch_loss=6.289211\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:52 INFO 140362387572544] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=6.289211273193359\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:52 INFO 140362387572544] Epoch[69] Batch[5] avg_epoch_loss=6.416458\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:52 INFO 140362387572544] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=6.416458209355672\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:52 INFO 140362387572544] Epoch[69] Batch [5]#011Speed: 581.08 samples/sec#011loss=6.416458\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:52 INFO 140362387572544] Epoch[69] Batch[10] avg_epoch_loss=5.956907\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:52 INFO 140362387572544] #quality_metric: host=algo-1, epoch=69, batch=10 train loss <loss>=5.405446195602417\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:52 INFO 140362387572544] Epoch[69] Batch [10]#011Speed: 575.94 samples/sec#011loss=5.405446\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:52 INFO 140362387572544] processed a total of 322 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422151.9678776, \"EndTime\": 1704422152.7158937, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 747.5483417510986, \"count\": 1, \"min\": 747.5483417510986, \"max\": 747.5483417510986}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:52 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=430.66994944140004 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:52 INFO 140362387572544] #progress_metric: host=algo-1, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:52 INFO 140362387572544] #quality_metric: host=algo-1, epoch=69, train loss <loss>=5.956907294013283\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:52 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:52 INFO 140362387572544] Epoch[70] Batch[0] avg_epoch_loss=5.594148\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:52 INFO 140362387572544] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=5.594147682189941\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:53 INFO 140362387572544] Epoch[70] Batch[5] avg_epoch_loss=5.539497\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:53 INFO 140362387572544] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=5.539496660232544\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:53 INFO 140362387572544] Epoch[70] Batch [5]#011Speed: 573.38 samples/sec#011loss=5.539497\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:53 INFO 140362387572544] Epoch[70] Batch[10] avg_epoch_loss=6.111823\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:53 INFO 140362387572544] #quality_metric: host=algo-1, epoch=70, batch=10 train loss <loss>=6.798614120483398\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:53 INFO 140362387572544] Epoch[70] Batch [10]#011Speed: 552.98 samples/sec#011loss=6.798614\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:53 INFO 140362387572544] processed a total of 323 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422152.71598, \"EndTime\": 1704422153.4755268, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 758.8441371917725, \"count\": 1, \"min\": 758.8441371917725, \"max\": 758.8441371917725}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:53 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=425.5564364161114 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:53 INFO 140362387572544] #progress_metric: host=algo-1, completed 71.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:53 INFO 140362387572544] #quality_metric: host=algo-1, epoch=70, train loss <loss>=6.111822778528387\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:53 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:53 INFO 140362387572544] Epoch[71] Batch[0] avg_epoch_loss=5.784707\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:53 INFO 140362387572544] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=5.7847065925598145\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:53 INFO 140362387572544] Epoch[71] Batch[5] avg_epoch_loss=5.533871\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:53 INFO 140362387572544] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=5.533871491750081\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:53 INFO 140362387572544] Epoch[71] Batch [5]#011Speed: 479.74 samples/sec#011loss=5.533871\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] Epoch[71] Batch[10] avg_epoch_loss=5.732767\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=5.971441555023193\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] Epoch[71] Batch [10]#011Speed: 557.44 samples/sec#011loss=5.971442\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] processed a total of 348 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422153.4756477, \"EndTime\": 1704422154.2836864, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 807.445764541626, \"count\": 1, \"min\": 807.445764541626, \"max\": 807.445764541626}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] #throughput_metric: host=algo-1, train throughput=430.92291604545363 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] #progress_metric: host=algo-1, completed 72.0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] #quality_metric: host=algo-1, epoch=71, train loss <loss>=5.732766975056041\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] Loading parameters from best epoch (31)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422154.2837706, \"EndTime\": 1704422154.289823, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.deserialize.time\": {\"sum\": 5.575895309448242, \"count\": 1, \"min\": 5.575895309448242, \"max\": 5.575895309448242}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] stopping training now\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] Final loss: 5.326172924041748 (occurred at epoch 31)\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] #quality_metric: host=algo-1, train final_loss <loss>=5.326172924041748\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 WARNING 140362387572544] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422154.2898784, \"EndTime\": 1704422154.3891792, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 98.4504222869873, \"count\": 1, \"min\": 98.4504222869873, \"max\": 98.4504222869873}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422154.3892424, \"EndTime\": 1704422154.4199047, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 129.21881675720215, \"count\": 1, \"min\": 129.21881675720215, \"max\": 129.21881675720215}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422154.419981, \"EndTime\": 1704422154.4248335, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 4.776716232299805, \"count\": 1, \"min\": 4.776716232299805, \"max\": 4.776716232299805}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] #memory_usage::<batchbuffer> = 7.3095703125 mb\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422154.4248862, \"EndTime\": 1704422154.431422, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.bind.time\": {\"sum\": 0.04458427429199219, \"count\": 1, \"min\": 0.04458427429199219, \"max\": 0.04458427429199219}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422154.4314966, \"EndTime\": 1704422154.6027336, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.score.time\": {\"sum\": 171.32806777954102, \"count\": 1, \"min\": 171.32806777954102, \"max\": 171.32806777954102}}}\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] #test_score (algo-1, RMSE): 5052657.757013453\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] #test_score (algo-1, mean_absolute_QuantileLoss): 167039777.64497885\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] #test_score (algo-1, mean_wQuantileLoss): 0.10690715014415669\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] #test_score (algo-1, wQuantileLoss[0.1]): 0.06934978829746394\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] #test_score (algo-1, wQuantileLoss[0.2]): 0.08731906779317063\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] #test_score (algo-1, wQuantileLoss[0.3]): 0.10367990691574187\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] #test_score (algo-1, wQuantileLoss[0.4]): 0.1167552676602344\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] #test_score (algo-1, wQuantileLoss[0.5]): 0.12675678768372922\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] #test_score (algo-1, wQuantileLoss[0.6]): 0.13612242168286975\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] #test_score (algo-1, wQuantileLoss[0.7]): 0.13260192085088338\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] #test_score (algo-1, wQuantileLoss[0.8]): 0.11204367589178194\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] #test_score (algo-1, wQuantileLoss[0.9]): 0.07753551452153491\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] #quality_metric: host=algo-1, test RMSE <loss>=5052657.757013453\u001b[0m\n",
      "\u001b[34m[01/05/2024 02:35:54 INFO 140362387572544] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.10690715014415669\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1704422154.6028278, \"EndTime\": 1704422154.618885, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 5.167245864868164, \"count\": 1, \"min\": 5.167245864868164, \"max\": 5.167245864868164}, \"totaltime\": {\"sum\": 54707.95679092407, \"count\": 1, \"min\": 54707.95679092407, \"max\": 54707.95679092407}}}\u001b[0m\n",
      "\n",
      "2024-01-05 02:36:14 Completed - Training job completed\n",
      "Training seconds: 337\n",
      "Billable seconds: 337\n"
     ]
    }
   ],
   "source": [
    "region = sess.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "s3_data_path = \"s3://{}/{}/\".format(bucket, prefix)\n",
    "s3_output_path = \"s3://{}/{}/output\".format(bucket, prefix)\n",
    "\n",
    "deepar_image = sagemaker.image_uris.retrieve(\"forecasting-deepar\", region)\n",
    "\n",
    "deepar = sagemaker.estimator.Estimator(\n",
    "    image_uri=deepar_image,\n",
    "    sagemaker_session=sess,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m4.xlarge',\n",
    "    base_job_name='deepar-forecast',\n",
    "    output_path=s3_output_path,\n",
    ")\n",
    "\n",
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"100\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"32\",\n",
    "    \"learning_rate\": \"1E-3\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "}\n",
    "\n",
    "deepar.set_hyperparameters(**hyperparameters)\n",
    "data_channels = {\"train\": \"{}train/\".format(s3_data_path), \"test\": \"{}test/\".format(s3_data_path)}\n",
    "\n",
    "deepar.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f223ef0-de2a-4b55-9fd7-d229c74d585b",
   "metadata": {},
   "source": [
    "### Deploy the Model\n",
    "\n",
    "Now, we need to deploy the model to an endpoint. As mentioned from the beginning, we need a lot of formatting in this part since this model requires JSON input and also outputs in JSON format. Fortunately, the cell below can be use as-is for other timeseries data, so let's not focus too much on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80cc07dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: deepar-forecast-2024-01-05-03-01-47-446\n",
      "INFO:sagemaker:Creating endpoint-config with name deepar-forecast-2024-01-05-03-01-47-446\n",
      "INFO:sagemaker:Creating endpoint with name deepar-forecast-2024-01-05-03-01-47-446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import IdentitySerializer\n",
    "\n",
    "class DeepARPredictor(sagemaker.predictor.Predictor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            *args,\n",
    "            serializer=IdentitySerializer(content_type=\"application/json\"),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def predict(self, ts, cat=None, dynamic_feat=None, num_samples=100, return_samples=False, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        prediction_time = ts.index[-1] + timedelta(days=1)\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, '1D', prediction_time, return_samples)\n",
    "\n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None)\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles,\n",
    "        }\n",
    "        http_request_data = {\"instances\": [instance], \"configuration\": configuration}\n",
    "        return json.dumps(http_request_data).encode(\"utf-8\")\n",
    "\n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        predictions = json.loads(response.decode(\"utf-8\"))[\"predictions\"][0]\n",
    "        prediction_length = len(next(iter(predictions[\"quantiles\"].values())))\n",
    "        prediction_index = pd.date_range(start=prediction_time, freq=freq, periods=prediction_length)\n",
    "        if return_samples:\n",
    "            dict_of_samples = {\"sample_\" + str(i): s for i, s in enumerate(predictions[\"samples\"])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(\n",
    "            data={**predictions[\"quantiles\"], **dict_of_samples}, index=prediction_index\n",
    "        )\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "\n",
    "\n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]\n",
    "\n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat\n",
    "    return obj\n",
    "    \n",
    "predictor = deepar.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\", predictor_cls=DeepARPredictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa22d298-dee1-48af-85c6-f4e9876b239a",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Finally, we can make prediction. Simply use `predict()` from the model, with `ts` being the column we want to make prediction. The result is the next `prediction_length` days from the last day in the provided data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7281a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_forecast = data['Open']\n",
    "forecast = predictor.predict(ts=to_forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7250f764-862e-4894-85b1-1934b3b853c6",
   "metadata": {},
   "source": [
    "The content of the prediction is a `pandas` dataframe with four columns: forecast date as index, lower confident interval, prediction, and upper confident interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "899421a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-12-30</th>\n",
       "      <td>451.498810</td>\n",
       "      <td>465.183167</td>\n",
       "      <td>476.321960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31</th>\n",
       "      <td>452.292297</td>\n",
       "      <td>464.272064</td>\n",
       "      <td>473.834320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-01</th>\n",
       "      <td>453.668610</td>\n",
       "      <td>466.451111</td>\n",
       "      <td>479.483704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-02</th>\n",
       "      <td>449.573029</td>\n",
       "      <td>465.694641</td>\n",
       "      <td>479.389709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-03</th>\n",
       "      <td>452.219666</td>\n",
       "      <td>463.741486</td>\n",
       "      <td>472.066559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0.1         0.5         0.9\n",
       "2023-12-30  451.498810  465.183167  476.321960\n",
       "2023-12-31  452.292297  464.272064  473.834320\n",
       "2024-01-01  453.668610  466.451111  479.483704\n",
       "2024-01-02  449.573029  465.694641  479.389709\n",
       "2024-01-03  452.219666  463.741486  472.066559"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec64430-c502-45b7-aee9-557c34594f1a",
   "metadata": {},
   "source": [
    "We can even draw the prediction. I will draw from 12/01/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98775c93-72bc-47ba-8717-2a1b9800b5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "to_plot = to_forecast['2023-12-01':]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(pd.concat([to_plot, forecast['0.5']]), linewidth=3)\n",
    "plt.plot(pd.concat([forecast['0.5']]), linewidth=3)\n",
    "plt.axvline(forecast.index[0], linestyle='--', color='red', linewidth=3, alpha=0.5)\n",
    "plt.fill_between([to_plot.index[-1]] + list(forecast.index), [to_plot[-1]] + list(forecast['0.1']), [to_plot[-1]] + list(forecast['0.9']), alpha=0.2)\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be75ad1-80d2-4f21-b14c-d5c8413d6737",
   "metadata": {},
   "source": [
    "Can you try to forecast and visualize the close price?\n",
    "\n",
    "To increase the forecast duration, we will need to go back to the beginning and train the model with higher `prediction_length`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8948217f-0389-4081-a2e3-bcd0b36239a8",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "**As usual, remember to clean up the model and endpoints to avoid any charges!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e4151e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: deepar-forecast-2024-01-05-03-01-47-446\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: deepar-forecast-2024-01-05-03-01-47-446\n",
      "INFO:sagemaker:Deleting endpoint with name: deepar-forecast-2024-01-05-03-01-47-446\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
